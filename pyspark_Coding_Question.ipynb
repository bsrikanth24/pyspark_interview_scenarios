{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b2bad440-32a2-491a-a32f-a3b5c615facf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cbcc85f5-8efc-4ec5-b54c-18f464856b21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Clean the Data & handle the Null Values\n",
    "2. Derive the highest average scored candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca0a293d-9b6e-4e0c-9ba7-7e5213dadf14",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "("
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------+-------------+-------------+\n|       student_id|math_score|reading_score|writing_score|\n+-----------------+----------+-------------+-------------+\n|ujLsq7296726iyzru|      75.0|         69.0|         68.0|\n|  zdcku34151cfkek|      null|         null|         null|\n|anlya8548116kgokl|      null|         80.0|         75.0|\n|ykucw6122786vejtt|      79.0|         78.0|         null|\n+-----------------+----------+-------------+-------------+\n\n+----------+-------------+\n|student_id|average_score|\n+----------+-------------+\n|   8548116|        77.33|\n+----------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg, col, when, regexp_replace, format_number\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName('TopStudents').getOrCreate()\n",
    "\n",
    "# Create the data dictionary\n",
    "data = {\n",
    "    'student_id': ['ujLsq7296726iyzru', 'zdcku34151cfkek', 'anlya8548116kgokl', 'ykucw6122786vejtt'],\n",
    "    'math_score': [75.0, None, None, 79.0],\n",
    "    'reading_score': [69.0, None, 80.0, 78.0],\n",
    "    'writing_score': [68.0, None, 75.0, None]\n",
    "}\n",
    "\n",
    "# Convert the data dictionary to a list of tuples\n",
    "data_list = list(zip(data['student_id'], data['math_score'], data['reading_score'], data['writing_score']))\n",
    "\n",
    "# Define the schema\n",
    "schema = 'student_id string, math_score float, reading_score float, writing_score float'\n",
    "\n",
    "# Create a DataFrame\n",
    "df = spark.createDataFrame(data_list, schema)\n",
    "\n",
    "# Display the DataFrame\n",
    "df.show()\n",
    "\n",
    "# Define the function to find the topper\n",
    "def find_the_topper(df):\n",
    "    # 1. Drop rows with more than 2 null values\n",
    "    df = df.withColumn(\"null_count\", \n",
    "                    when(col('math_score').isNull(), 1).otherwise(0) + \n",
    "                    when(col('reading_score').isNull(), 1).otherwise(0) + \n",
    "                    when(col('writing_score').isNull(), 1).otherwise(0))\n",
    "\n",
    "    df = df.filter(col(\"null_count\") < 2).drop(\"null_count\")\n",
    "\n",
    "    # Calculate the average scores for each subject\n",
    "    math_average = df.agg(avg(col('math_score'))).collect()[0][0]\n",
    "    reading_score_average = df.agg(avg(col('reading_score'))).collect()[0][0]\n",
    "    writing_score_average = df.agg(avg(col('writing_score'))).collect()[0][0]\n",
    "\n",
    "    # Fill the missing values with the calculated averages\n",
    "    df = df.fillna({'math_score': math_average, 'reading_score': reading_score_average, 'writing_score': writing_score_average})\n",
    "\n",
    "    # 3. Cleaning of student_id\n",
    "    df = df.withColumn(\"student_id\", regexp_replace(col(\"student_id\"), r'\\D', ''))\n",
    "\n",
    "    # 4. Find the average score and format it to 2 decimal places\n",
    "    df = df.withColumn(\"average_score\", \n",
    "                   format_number((col('math_score') + col('reading_score') + col('writing_score')) / 3, 2))\n",
    "\n",
    "    # 5. Find the student with the highest average score\n",
    "    topper = df.select(\"student_id\", \"average_score\").orderBy(col(\"average_score\").desc()).limit(1)\n",
    "\n",
    "    return topper\n",
    "\n",
    "# Run the function and display the result\n",
    "topper_df = find_the_topper(df)\n",
    "topper_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5dae1f1a-8828-4abb-9077-0dc258924678",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6bb770a1-7108-4563-86f1-5aab141c0ba6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Clean the Data & handle the Null Values using approxQuantile\n",
    "2. Derive the highest average scored candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1eb3124e-c409-4b63-aed3-fad332165a5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------+-------------+-------------+\n|       student_id|math_score|reading_score|writing_score|\n+-----------------+----------+-------------+-------------+\n|ujLsq7296726iyzru|      75.0|         69.0|         68.0|\n|  zdcku34151cfkek|      null|         null|         null|\n|anlya8548116kgokl|      null|         80.0|         75.0|\n|ykucw6122786vejtt|      79.0|         78.0|         null|\n+-----------------+----------+-------------+-------------+\n\n+----------+-------------+\n|student_id|average_score|\n+----------+-------------+\n|   8548116|        76.67|\n+----------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, regexp_replace, format_number\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName('TopStudents').getOrCreate()\n",
    "\n",
    "# Create the data dictionary\n",
    "data = {\n",
    "    'student_id': ['ujLsq7296726iyzru', 'zdcku34151cfkek', 'anlya8548116kgokl', 'ykucw6122786vejtt'],\n",
    "    'math_score': [75.0, None, None, 79.0],\n",
    "    'reading_score': [69.0, None, 80.0, 78.0],\n",
    "    'writing_score': [68.0, None, 75.0, None]\n",
    "}\n",
    "\n",
    "# Convert the data dictionary to a list of tuples\n",
    "data_list = list(zip(data['student_id'], data['math_score'], data['reading_score'], data['writing_score']))\n",
    "\n",
    "# Define the schema\n",
    "schema = 'student_id string, math_score float, reading_score float, writing_score float'\n",
    "\n",
    "# Create a DataFrame\n",
    "df = spark.createDataFrame(data_list, schema)\n",
    "\n",
    "# Display the DataFrame\n",
    "df.show()\n",
    "\n",
    "# Define the function to find the topper\n",
    "def find_the_topper(df):\n",
    "    # 1. Drop rows with more than 2 null values\n",
    "    df = df.withColumn(\"null_count\", \n",
    "                    when(col('math_score').isNull(), 1).otherwise(0) + \n",
    "                    when(col('reading_score').isNull(), 1).otherwise(0) + \n",
    "                    when(col('writing_score').isNull(), 1).otherwise(0))\n",
    "\n",
    "    df = df.filter(col('null_count') < 2).drop(\"null_count\")\n",
    "\n",
    "    # Calculate the median scores for each subject\n",
    "    def calculate_median(df, col_name):\n",
    "        return df.approxQuantile(col_name, [0.5], 0.0)[0]\n",
    "    \n",
    "    math_median = calculate_median(df, 'math_score')\n",
    "    reading_score_median = calculate_median(df, 'reading_score')\n",
    "    writing_score_median = calculate_median(df, 'writing_score')\n",
    "\n",
    "    # Fill the missing values with the calculated medians\n",
    "    df = df.fillna({'math_score': math_median, 'reading_score': reading_score_median, 'writing_score': writing_score_median})\n",
    "\n",
    "    # 3. Cleaning of student_id\n",
    "    df = df.withColumn(\"student_id\", regexp_replace(col(\"student_id\"), r'\\D', ''))\n",
    "\n",
    "    # 4. Find the average score and format it to 2 decimal places\n",
    "    df = df.withColumn(\"average_score\", \n",
    "                   format_number((col('math_score') + col('reading_score') + col('writing_score')) / 3, 2))\n",
    "\n",
    "    # 5. Find the student with the highest average score\n",
    "    topper = df.select(\"student_id\", \"average_score\").orderBy(col(\"average_score\").desc()).limit(1)\n",
    "\n",
    "    return topper\n",
    "\n",
    "# Run the function and display the result\n",
    "topper_df = find_the_topper(df)\n",
    "topper_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b077ea0c-8ce2-4030-bf07-ae2a0b0e1808",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "02fde524-3efc-4c12-8681-3d0c723b1481",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. date format Handling \n",
    "2. case statement in pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b48ba6af-6305-43c4-8fcc-86f8e3cd8d36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----------+------+------+\n|firstname|middlename|lastname|       dob|gender|salary|\n+---------+----------+--------+----------+------+------+\n|    James|          |   Smith|1994-04-01|     M|  3000|\n|  Michael|          |    Rose|2000-05-19|     M|  3500|\n|   Robert|          |Williams|1978-09-05|     M|  3500|\n|    Maria|      Anne|   Jones|1967-12-01|     F|  3500|\n|      Jen|      Mary|   Brown|1980-02-17|     F|     1|\n+---------+----------+--------+----------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Corrected data list\n",
    "data = [\n",
    "    ('James', '', 'Smith', '1994-04-01', 'M', 3000),\n",
    "    ('Michael', '', 'Rose', '2000-05-19', 'M', 4000),\n",
    "    ('Robert', '', 'Williams', '1978-09-05', 'M', 4000),\n",
    "    ('Maria', 'Anne', 'Jones', '1967-12-01', 'F', 4000),\n",
    "    ('Jen', 'Mary', 'Brown', '1980-02-17', 'F', -1)\n",
    "]\n",
    "\n",
    "# Corrected columns list (as a list)\n",
    "columns = [\"firstname\", \"middlename\", \"lastname\", \"dob\", \"gender\", \"salary\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "df = df.withColumn(\"dob\", to_date(\"dob\")).withColumn(\"salary\", col(\"salary\").cast(\"int\"))\n",
    "\n",
    "df = df.withColumn(\"salary\", \n",
    "                   when(col(\"salary\") == -1, 1)\n",
    "                   .when(col(\"salary\") == 4000, 3500)\n",
    "                   .otherwise(col(\"salary\")))\n",
    "\n",
    "# Show DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "801c5e86-3d57-48da-8327-90e5a9970282",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f1f916a-7640-4731-b8c1-5d99695f69e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Configuration on pyspark for resource optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dfad7ed5-5231-42f4-b771-8ea6773a4a5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import getpass\n",
    "\n",
    "# Get the current user's name\n",
    "username = getpass.getuser()\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        # Provide a name for your application\n",
    "        .appName(\"Spark_API\") \\\n",
    "        # Set the UI port to avoid conflicts\n",
    "        .config('spark.ui.port', '0') \\\n",
    "        # Specify the warehouse directory for metastore.\n",
    "        .config('spark.sql.warehouse.dir', '<path to save metastore details>') \\\n",
    "        # Disable dynamic allocation of resources\n",
    "        .config('spark.sql.dynamicAllocation.enabled', 'false') \\\n",
    "        # Set the number of executor instances\n",
    "        .config('spark.executor.instances', '4') \\\n",
    "        # Set the number of cores per executor\n",
    "        .config('spark.executor.cores', '2') \\\n",
    "        # Set the memory allocated to each executor\n",
    "        .config('spark.executor.memory', '4G') \\\n",
    "        # Enable off-heap memory\n",
    "        .config('spark.memory.offHeap.enabled', 'true') \\\n",
    "        # Set the amount of off-heap memory\n",
    "        .config('spark.memory.offHeap.size', '2G') \\\n",
    "        # Set the number of cores for the driver\n",
    "        .config('spark.driver.cores', '4') \\\n",
    "        # Set the memory allocated to the driver\n",
    "        .config('spark.driver.memory', '6G') \\\n",
    "        # Set the fraction of memory for storage\n",
    "        .config('spark.storage.fraction', '0.7') \\\n",
    "        # Set the fraction of memory allocated to storage after execution\n",
    "        .config('spark.memory.storageFraction', '0.6') \\\n",
    "        # Enable Hive support for Spark\n",
    "        .enableHiveSupport() \\\n",
    "        # Set the master to YARN for resource management\n",
    "        .master('yarn') \\\n",
    "        # Create the Spark session\n",
    "        .getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3fe0b13a-2eb9-42ff-aa87-f08c40b4293b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb12c0c8-b2f8-4474-ab10-9c34361a9819",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Code for spark job Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "17f69980-889a-41f0-b7ae-413c0d3612cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import getpass\n",
    "\n",
    "# Get the current user's name\n",
    "username = getpass.getuser()\n",
    "\n",
    "# Initialize a Spark session with Hive support\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .enableHiveSupport() \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Your main Spark application code goes here\n",
    "\n",
    "# Submit the Spark job with the following configurations\n",
    "spark-submit \\\n",
    "  # Specify the cluster manager as YARN\n",
    "  --master yarn \\\n",
    "  # Set the deploy mode to cluster (runs the driver on a YARN cluster node)\n",
    "  --deploy-mode cluster \\\n",
    "  # Enable verbose logging for detailed output\n",
    "  --verbose \\\n",
    "  # Set the Spark UI port to avoid conflicts\n",
    "  --conf spark.ui.port=0 \\\n",
    "  # Disable dynamic resource allocation\n",
    "  --conf spark.sql.dynamicAllocation.enabled=false \\\n",
    "  # Specify the number of executor instances\n",
    "  --conf spark.executor.instances=4 \\\n",
    "  # Set the number of cores per executor\n",
    "  --conf spark.executor.cores=2 \\\n",
    "  # Allocate memory for each executor\n",
    "  --conf spark.executor.memory=4G \\\n",
    "  # Enable off-heap memory usage\n",
    "  --conf spark.memory.offHeap.enabled=true \\\n",
    "  # Set the size of off-heap memory\n",
    "  --conf spark.memory.offHeap.size=2G \\\n",
    "  # Configure the number of cores for the driver\n",
    "  --conf spark.driver.cores=4 \\\n",
    "  # Allocate memory for the driver\n",
    "  --conf spark.driver.memory=6G \\\n",
    "  # Determine the fraction of memory used for storage\n",
    "  --conf spark.storage.fraction=0.7 \\\n",
    "  # Set the fraction of memory allocated for storage after execution\n",
    "  --conf spark.memory.storageFraction=0.6 \\\n",
    "  # Specify the Python script to run\n",
    "  example_script.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9c96a21-96a8-4919-8b84-fe69aad9b5aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark-submit \\\n",
    "--name Spark_API \\\n",
    "--master yarn \\\n",
    "--conf spark.ui.port=0 \\\n",
    "--conf spark.sql.warehouse.dir=<path to save metastore details> \\\n",
    "--conf spark.sql.dynamicAllocation.enabled=false \\\n",
    "--conf spark.executor.instances=4 \\\n",
    "--conf spark.executor.cores=2 \\\n",
    "--conf spark.executor.memory=4G \\\n",
    "--conf spark.memory.offHeap.enabled=true \\\n",
    "--conf spark.memory.offHeap.size=2G \\\n",
    "--conf spark.driver.cores=4 \\\n",
    "--conf spark.driver.memory=6G \\\n",
    "--conf spark.storage.fraction=0.7 \\\n",
    "--conf spark.memory.storageFraction=0.6 \\\n",
    "--conf spark.sql.catalogImplementation=hive \\\n",
    "spark_app.py\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39314f3a-83b7-4c67-aece-e9949d3466f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Question 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e67bce5d-e346-41a6-bd34-a4d0ed1e3f76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. find the different joins output for given table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84c8e3d2-3a75-4acb-8dd7-2a5744a17f79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>num_affected_rows</th><th>num_inserted_rows</th></tr></thead><tbody><tr><td>5</td><td>5</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         5,
         5
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "num_affected_rows",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "num_inserted_rows",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "CREATE TABLE TableA (\n",
    "    id INT\n",
    ");\n",
    "\n",
    "CREATE TABLE TableB (\n",
    "    id INT\n",
    ");\n",
    "\n",
    "-- Insert data into TableA\n",
    "INSERT INTO TableA (id) VALUES (1), (1), (1), (0);\n",
    "\n",
    "-- Insert data into TableB\n",
    "INSERT INTO TableB (id) VALUES (1), (1), (0), (1), (NULL);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d2cf93b6-d0a1-4a2d-bbdc-28bce45be0aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Question 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "86f525e4-a717-4beb-844d-8b5f1bbb1d93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Count the NULL values per column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5430d960-9455-4790-9f9c-82248d3be42d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n|  id|   name|\n+----+-------+\n|   1|  Alice|\n|   2|   null|\n|null|    Bob|\n|   3|Charlie|\n|null|   null|\n+----+-------+\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>count(CASE WHEN (id IS NULL) THEN id END AS id)</th><th>count(CASE WHEN (name IS NULL) THEN name END AS name)</th></tr></thead><tbody><tr><td>2</td><td>2</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         2,
         2
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "count(CASE WHEN (id IS NULL) THEN id END AS id)",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "count(CASE WHEN (name IS NULL) THEN name END AS name)",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"CreateDataFrameWithNulls\").getOrCreate()\n",
    "\n",
    "# Define the schema with two columns\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Sample data with null values\n",
    "data = [\n",
    "    (1, \"Alice\"),\n",
    "    (2, None),      # NULL in the 'name' column\n",
    "    (None, \"Bob\"),  # NULL in the 'id' column\n",
    "    (3, \"Charlie\"),\n",
    "    (None, None)    # NULL in both columns\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "df_1 = df.select([count(when(col(i).isNull(), i).alias(i)) for i in df.columns])\n",
    "display(df_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "17d4773c-2b1a-4e53-8c12-c296f64f0811",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Question 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f992765-c3e9-444a-8a48-01664b12cabc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. convert the multiple delimeter file into structured table format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a5d2ab4-a0ac-414b-960d-7ce824e9314f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 103 bytes.\nOut[6]: True"
     ]
    }
   ],
   "source": [
    "dbutils.fs.put(\"dbfs:/FileStore/delimiter_problem.txt\", \"\"\"\n",
    "               name~|age\\nfaizan,mohd~|28\\nfurqan.mohd~|28\\nali,akbar~|22\\nzeesha,mohd~|32\n",
    "               \"\"\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6aedfc21-499e-478c-9962-0c3b73c7415f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>split_col</th></tr></thead><tbody><tr><td>List(name, age, city, country)</td></tr><tr><td>List(John, 25, New York, USA)</td></tr><tr><td>List(Jane, 30, London, UK)</td></tr><tr><td>List(Doe, 22, Sydney, Australia)</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         [
          "name",
          "age",
          "city",
          "country"
         ]
        ],
        [
         [
          "John",
          "25",
          "New York",
          "USA"
         ]
        ],
        [
         [
          "Jane",
          "30",
          "London",
          "UK"
         ]
        ],
        [
         [
          "Doe",
          "22",
          "Sydney",
          "Australia"
         ]
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "split_col",
         "type": "{\"type\":\"array\",\"elementType\":\"string\",\"containsNull\":false}"
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df= spark.read.text('dbfs:/FileStore/workout_data/delimeter.txt')\n",
    "split_df = df.select(\n",
    "    split(col(\"value\"), r\"[|,;]\").alias(\"split_col\")\n",
    ")\n",
    "display(split_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2371a634-207c-4adc-83fe-17464141fbfb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+--------+---------+\n|name|age|    city|  country|\n+----+---+--------+---------+\n|name|age|    city|  country|\n|John| 25|New York|      USA|\n|Jane| 30|  London|       UK|\n| Doe| 22|  Sydney|Australia|\n+----+---+--------+---------+\n\n+----+---+--------+---------+\n|name|age|    city|  country|\n+----+---+--------+---------+\n| Doe| 22|  Sydney|Australia|\n|John| 25|New York|      USA|\n|Jane| 30|  London|       UK|\n+----+---+--------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "final_df = split_df.select(\n",
    "    col(\"split_col\").getItem(0).alias(\"name\"),\n",
    "    col(\"split_col\").getItem(1).alias(\"age\"),\n",
    "    col(\"split_col\").getItem(2).alias(\"city\"),\n",
    "    col(\"split_col\").getItem(3).alias(\"country\")\n",
    ")\n",
    "\n",
    "final_df.show()\n",
    "first_row_df = final_df.limit(1)\n",
    "\n",
    "# Filter out the first row to get the remaining rows\n",
    "remaining_rows_df = final_df.exceptAll(first_row_df)\n",
    "\n",
    "remaining_rows_df = final_df.where(final_df != first_row_df )\n",
    "# Show the remaining rows\n",
    "remaining_rows_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14bba687-283a-4e90-acf6-60ce78531a85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Question 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94aa84aa-0094-4811-aa71-a66a33416064",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Remove Spaces from the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62c3b21e-9a16-461c-a358-efa9f82fc08c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+----------+------+\n|order_/id|customer name|  pro@duct|amount|\n+---------+-------------+----------+------+\n|        1|     John Doe|    Laptop|1000.5|\n|        2|   Jane Smith|Smartphone| 750.0|\n|        3|    Sam Brown|    Tablet|300.25|\n|        4|  Emily Jones|Headphones|150.75|\n|        5|  Michael Lee|   Monitor| 200.0|\n+---------+-------------+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType\n",
    "\n",
    "# Define the schema\n",
    "schema = StructType([\n",
    "    StructField(\"order_/id\", IntegerType(), True),\n",
    "    StructField(\"customer name\", StringType(), True),\n",
    "    StructField(\"pro@duct\", StringType(), True),\n",
    "    StructField(\"amount\", FloatType(), True)\n",
    "])\n",
    "\n",
    "# Create the data\n",
    "data = [\n",
    "    (1, \"John Doe\", \"Laptop\", 1000.50),\n",
    "    (2, \"Jane Smith\", \"Smartphone\", 750.00),\n",
    "    (3, \"Sam Brown\", \"Tablet\", 300.25),\n",
    "    (4, \"Emily Jones\", \"Headphones\", 150.75),\n",
    "    (5, \"Michael Lee\", \"Monitor\", 200.00)\n",
    "]\n",
    "\n",
    "# Create the DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3db9a42b-05b3-4b20-9087-007d35209c27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>order__id</th><th>customer_name</th><th>pro_duct</th><th>amount</th></tr></thead><tbody><tr><td>1</td><td>John Doe</td><td>Laptop</td><td>1000.5</td></tr><tr><td>2</td><td>Jane Smith</td><td>Smartphone</td><td>750.0</td></tr><tr><td>3</td><td>Sam Brown</td><td>Tablet</td><td>300.25</td></tr><tr><td>4</td><td>Emily Jones</td><td>Headphones</td><td>150.75</td></tr><tr><td>5</td><td>Michael Lee</td><td>Monitor</td><td>200.0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "John Doe",
         "Laptop",
         1000.5
        ],
        [
         2,
         "Jane Smith",
         "Smartphone",
         750.0
        ],
        [
         3,
         "Sam Brown",
         "Tablet",
         300.25
        ],
        [
         4,
         "Emily Jones",
         "Headphones",
         150.75
        ],
        [
         5,
         "Michael Lee",
         "Monitor",
         200.0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "order__id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "customer_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "pro_duct",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "amount",
         "type": "\"float\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "def correct_column_names(df):\n",
    "    corrected_columns = [\n",
    "        re.sub(r'[^a-zA-Z0-9_]', '_', column).strip('_') for column in df.columns\n",
    "    ]\n",
    "    return df.toDF(*corrected_columns)\n",
    "\n",
    "df_corrected = correct_column_names(df)\n",
    "\n",
    "display(df_corrected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de8e43bc-5199-449d-83ee-b74c9c245a09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Question 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a068ecc4-5fac-43ee-9f7d-269a55dd33ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Handeling of Nested JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "262cff95-033e-45b0-b508-e334a22539b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 1106 bytes.\nOut[52]: True"
     ]
    }
   ],
   "source": [
    "dbutils.fs.put(\"dbfs:/FileStore/workout_data/nested.json\", \"\"\" \n",
    "               {\n",
    "  \"name\":\"MSFT\",\"location\":\"Redmond\", \"satellites\": [\"Bay Area\", \"Shanghai\"],\n",
    "  \"goods\": {\n",
    "    \"trade\":true, \"customers\":[\"government\", \"distributer\", \"retail\"],\n",
    "    \"orders\":[\n",
    "        {\"orderId\":1,\"orderTotal\":123.34,\"shipped\":{\"orderItems\":[{\"itemName\":\"Laptop\",\"itemQty\":20},{\"itemName\":\"Charger\",\"itemQty\":2}]}},\n",
    "        {\"orderId\":2,\"orderTotal\":323.34,\"shipped\":{\"orderItems\":[{\"itemName\":\"Mice\",\"itemQty\":2},{\"itemName\":\"Keyboard\",\"itemQty\":1}]}}\n",
    "    ]}}\n",
    "{\"name\":\"Company1\",\"location\":\"Seattle\", \"satellites\": [\"New York\"],\n",
    "  \"goods\":{\"trade\":false, \"customers\":[\"store1\", \"store2\"],\n",
    "  \"orders\":[\n",
    "      {\"orderId\":4,\"orderTotal\":123.34,\"shipped\":{\"orderItems\":[{\"itemName\":\"Laptop\",\"itemQty\":20},{\"itemName\":\"Charger\",\"itemQty\":3}]}},\n",
    "      {\"orderId\":5,\"orderTotal\":343.24,\"shipped\":{\"orderItems\":[{\"itemName\":\"Chair\",\"itemQty\":4},{\"itemName\":\"Lamp\",\"itemQty\":2}]}}\n",
    "    ]}}\n",
    "{\"name\": \"Company2\", \"location\": \"Bellevue\",\n",
    "  \"goods\": {\"trade\": true, \"customers\":[\"Bank\"], \"orders\": [{\"orderId\": 4, \"orderTotal\": 123.34}]}}\n",
    "{\"name\": \"Company3\", \"location\": \"Kirkland\"}\n",
    "               \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "370a06c6-f810-443c-a82e-da1135ae46f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>goods</th><th>location</th><th>name</th><th>satellites</th></tr></thead><tbody><tr><td>List(List(government, distributer, retail), List(List(1, 123.34, List(List(List(Laptop, 20), List(Charger, 2)))), List(2, 323.34, List(List(List(Mice, 2), List(Keyboard, 1))))), true)</td><td>Redmond</td><td>MSFT</td><td>List(Bay Area, Shanghai)</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         [
          [
           "government",
           "distributer",
           "retail"
          ],
          [
           [
            1,
            123.34,
            [
             [
              [
               "Laptop",
               20
              ],
              [
               "Charger",
               2
              ]
             ]
            ]
           ],
           [
            2,
            323.34,
            [
             [
              [
               "Mice",
               2
              ],
              [
               "Keyboard",
               1
              ]
             ]
            ]
           ]
          ],
          true
         ],
         "Redmond",
         "MSFT",
         [
          "Bay Area",
          "Shanghai"
         ]
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "goods",
         "type": "{\"type\":\"struct\",\"fields\":[{\"name\":\"customers\",\"type\":{\"type\":\"array\",\"elementType\":\"string\",\"containsNull\":true},\"nullable\":true,\"metadata\":{}},{\"name\":\"orders\",\"type\":{\"type\":\"array\",\"elementType\":{\"type\":\"struct\",\"fields\":[{\"name\":\"orderId\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"orderTotal\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"shipped\",\"type\":{\"type\":\"struct\",\"fields\":[{\"name\":\"orderItems\",\"type\":{\"type\":\"array\",\"elementType\":{\"type\":\"struct\",\"fields\":[{\"name\":\"itemName\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"itemQty\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}}]},\"containsNull\":true},\"nullable\":true,\"metadata\":{}}]},\"nullable\":true,\"metadata\":{}}]},\"containsNull\":true},\"nullable\":true,\"metadata\":{}},{\"name\":\"trade\",\"type\":\"boolean\",\"nullable\":true,\"metadata\":{}}]}"
        },
        {
         "metadata": "{}",
         "name": "location",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "satellites",
         "type": "{\"type\":\"array\",\"elementType\":\"string\",\"containsNull\":true}"
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- goods: struct (nullable = true)\n |    |-- customers: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |    |-- orders: array (nullable = true)\n |    |    |-- element: struct (containsNull = true)\n |    |    |    |-- orderId: long (nullable = true)\n |    |    |    |-- orderTotal: double (nullable = true)\n |    |    |    |-- shipped: struct (nullable = true)\n |    |    |    |    |-- orderItems: array (nullable = true)\n |    |    |    |    |    |-- element: struct (containsNull = true)\n |    |    |    |    |    |    |-- itemName: string (nullable = true)\n |    |    |    |    |    |    |-- itemQty: long (nullable = true)\n |    |-- trade: boolean (nullable = true)\n |-- location: string (nullable = true)\n |-- name: string (nullable = true)\n |-- satellites: array (nullable = true)\n |    |-- element: string (containsNull = true)\n\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"multiLine\", \"true\").json(\"dbfs:/FileStore/workout_data/nested.json\")\n",
    "display(df)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2e06111-85ef-4950-912a-25e2762f6df7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "def child_struct(nested_df):\n",
    "    # Creating python set to store dataframe metadata\n",
    "    list_schema = [((), nested_df)]\n",
    "    \n",
    "    # Creating empty python list for final flattened columns\n",
    "    flat_columns = []\n",
    "\n",
    "    # Looping until there are no more schemas to process\n",
    "    while len(list_schema) > 0:\n",
    "        # Removing the latest or recently added item (dataframe schema) and returning it into the df variable\n",
    "        parents, df = list_schema.pop()\n",
    "        \n",
    "        # Creating columns for non-struct fields\n",
    "        flat_cols = [\n",
    "            col(\".\".join(parents + (c[0],))).alias(\"_\".join(parents + (c[0],)))\n",
    "            for c in df.dtypes if c[1][:6] != \"struct\"\n",
    "        ]\n",
    "        \n",
    "        # Identifying columns that are of struct type\n",
    "        struct_cols = [c[0] for c in df.dtypes if c[1][:6] == \"struct\"]\n",
    "        \n",
    "        # Adding flat columns to the flat_columns list\n",
    "        flat_columns.extend(flat_cols)\n",
    "        \n",
    "        # Reading nested columns and appending into the stack list\n",
    "        for i in struct_cols:\n",
    "            projected_df = df.select(i + \".*\")\n",
    "            list_schema.append((parents + (i,), projected_df))\n",
    "    \n",
    "    # Returning the flattened DataFrame with all columns\n",
    "    return nested_df.select(flat_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2de11cc8-ba57-4b98-a9d9-dba66669bdcd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode_outer\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "def master_array(df: DataFrame) -> DataFrame:\n",
    "    # Get initial list of array columns\n",
    "    array_cols = [c[0] for c in df.dtypes if c[1].startswith(\"array\")]\n",
    "    \n",
    "    while len(array_cols) > 0:\n",
    "        for c in array_cols:\n",
    "            df = df.withColumn(c, explode_outer(col(c)))\n",
    "        \n",
    "        # Update the list of array columns after the explosion\n",
    "        # Assume child_struct is a function that handles additional struct flattening\n",
    "        df = child_struct(df)\n",
    "        \n",
    "        # Get the updated list of array columns\n",
    "        array_cols = [c[0] for c in df.dtypes if c[1].startswith(\"array\")]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0215c32f-6e33-46ed-8309-8c8da3abdf8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>location</th><th>name</th><th>satellites</th><th>goods_customers</th><th>goods_trade</th><th>goods_orders_orderId</th><th>goods_orders_orderTotal</th><th>goods_orders_shipped_orderItems_itemName</th><th>goods_orders_shipped_orderItems_itemQty</th></tr></thead><tbody><tr><td>Redmond</td><td>MSFT</td><td>Bay Area</td><td>government</td><td>true</td><td>1</td><td>123.34</td><td>Laptop</td><td>20</td></tr><tr><td>Redmond</td><td>MSFT</td><td>Bay Area</td><td>government</td><td>true</td><td>1</td><td>123.34</td><td>Charger</td><td>2</td></tr><tr><td>Redmond</td><td>MSFT</td><td>Bay Area</td><td>government</td><td>true</td><td>2</td><td>323.34</td><td>Mice</td><td>2</td></tr><tr><td>Redmond</td><td>MSFT</td><td>Bay Area</td><td>government</td><td>true</td><td>2</td><td>323.34</td><td>Keyboard</td><td>1</td></tr><tr><td>Redmond</td><td>MSFT</td><td>Bay Area</td><td>distributer</td><td>true</td><td>1</td><td>123.34</td><td>Laptop</td><td>20</td></tr><tr><td>Redmond</td><td>MSFT</td><td>Bay Area</td><td>distributer</td><td>true</td><td>1</td><td>123.34</td><td>Charger</td><td>2</td></tr><tr><td>Redmond</td><td>MSFT</td><td>Bay Area</td><td>distributer</td><td>true</td><td>2</td><td>323.34</td><td>Mice</td><td>2</td></tr><tr><td>Redmond</td><td>MSFT</td><td>Bay Area</td><td>distributer</td><td>true</td><td>2</td><td>323.34</td><td>Keyboard</td><td>1</td></tr><tr><td>Redmond</td><td>MSFT</td><td>Bay Area</td><td>retail</td><td>true</td><td>1</td><td>123.34</td><td>Laptop</td><td>20</td></tr><tr><td>Redmond</td><td>MSFT</td><td>Bay Area</td><td>retail</td><td>true</td><td>1</td><td>123.34</td><td>Charger</td><td>2</td></tr><tr><td>Redmond</td><td>MSFT</td><td>Bay Area</td><td>retail</td><td>true</td><td>2</td><td>323.34</td><td>Mice</td><td>2</td></tr><tr><td>Redmond</td><td>MSFT</td><td>Bay Area</td><td>retail</td><td>true</td><td>2</td><td>323.34</td><td>Keyboard</td><td>1</td></tr><tr><td>Redmond</td><td>MSFT</td><td>Shanghai</td><td>government</td><td>true</td><td>1</td><td>123.34</td><td>Laptop</td><td>20</td></tr><tr><td>Redmond</td><td>MSFT</td><td>Shanghai</td><td>government</td><td>true</td><td>1</td><td>123.34</td><td>Charger</td><td>2</td></tr><tr><td>Redmond</td><td>MSFT</td><td>Shanghai</td><td>government</td><td>true</td><td>2</td><td>323.34</td><td>Mice</td><td>2</td></tr><tr><td>Redmond</td><td>MSFT</td><td>Shanghai</td><td>government</td><td>true</td><td>2</td><td>323.34</td><td>Keyboard</td><td>1</td></tr><tr><td>Redmond</td><td>MSFT</td><td>Shanghai</td><td>distributer</td><td>true</td><td>1</td><td>123.34</td><td>Laptop</td><td>20</td></tr><tr><td>Redmond</td><td>MSFT</td><td>Shanghai</td><td>distributer</td><td>true</td><td>1</td><td>123.34</td><td>Charger</td><td>2</td></tr><tr><td>Redmond</td><td>MSFT</td><td>Shanghai</td><td>distributer</td><td>true</td><td>2</td><td>323.34</td><td>Mice</td><td>2</td></tr><tr><td>Redmond</td><td>MSFT</td><td>Shanghai</td><td>distributer</td><td>true</td><td>2</td><td>323.34</td><td>Keyboard</td><td>1</td></tr><tr><td>Redmond</td><td>MSFT</td><td>Shanghai</td><td>retail</td><td>true</td><td>1</td><td>123.34</td><td>Laptop</td><td>20</td></tr><tr><td>Redmond</td><td>MSFT</td><td>Shanghai</td><td>retail</td><td>true</td><td>1</td><td>123.34</td><td>Charger</td><td>2</td></tr><tr><td>Redmond</td><td>MSFT</td><td>Shanghai</td><td>retail</td><td>true</td><td>2</td><td>323.34</td><td>Mice</td><td>2</td></tr><tr><td>Redmond</td><td>MSFT</td><td>Shanghai</td><td>retail</td><td>true</td><td>2</td><td>323.34</td><td>Keyboard</td><td>1</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Redmond",
         "MSFT",
         "Bay Area",
         "government",
         true,
         1,
         123.34,
         "Laptop",
         20
        ],
        [
         "Redmond",
         "MSFT",
         "Bay Area",
         "government",
         true,
         1,
         123.34,
         "Charger",
         2
        ],
        [
         "Redmond",
         "MSFT",
         "Bay Area",
         "government",
         true,
         2,
         323.34,
         "Mice",
         2
        ],
        [
         "Redmond",
         "MSFT",
         "Bay Area",
         "government",
         true,
         2,
         323.34,
         "Keyboard",
         1
        ],
        [
         "Redmond",
         "MSFT",
         "Bay Area",
         "distributer",
         true,
         1,
         123.34,
         "Laptop",
         20
        ],
        [
         "Redmond",
         "MSFT",
         "Bay Area",
         "distributer",
         true,
         1,
         123.34,
         "Charger",
         2
        ],
        [
         "Redmond",
         "MSFT",
         "Bay Area",
         "distributer",
         true,
         2,
         323.34,
         "Mice",
         2
        ],
        [
         "Redmond",
         "MSFT",
         "Bay Area",
         "distributer",
         true,
         2,
         323.34,
         "Keyboard",
         1
        ],
        [
         "Redmond",
         "MSFT",
         "Bay Area",
         "retail",
         true,
         1,
         123.34,
         "Laptop",
         20
        ],
        [
         "Redmond",
         "MSFT",
         "Bay Area",
         "retail",
         true,
         1,
         123.34,
         "Charger",
         2
        ],
        [
         "Redmond",
         "MSFT",
         "Bay Area",
         "retail",
         true,
         2,
         323.34,
         "Mice",
         2
        ],
        [
         "Redmond",
         "MSFT",
         "Bay Area",
         "retail",
         true,
         2,
         323.34,
         "Keyboard",
         1
        ],
        [
         "Redmond",
         "MSFT",
         "Shanghai",
         "government",
         true,
         1,
         123.34,
         "Laptop",
         20
        ],
        [
         "Redmond",
         "MSFT",
         "Shanghai",
         "government",
         true,
         1,
         123.34,
         "Charger",
         2
        ],
        [
         "Redmond",
         "MSFT",
         "Shanghai",
         "government",
         true,
         2,
         323.34,
         "Mice",
         2
        ],
        [
         "Redmond",
         "MSFT",
         "Shanghai",
         "government",
         true,
         2,
         323.34,
         "Keyboard",
         1
        ],
        [
         "Redmond",
         "MSFT",
         "Shanghai",
         "distributer",
         true,
         1,
         123.34,
         "Laptop",
         20
        ],
        [
         "Redmond",
         "MSFT",
         "Shanghai",
         "distributer",
         true,
         1,
         123.34,
         "Charger",
         2
        ],
        [
         "Redmond",
         "MSFT",
         "Shanghai",
         "distributer",
         true,
         2,
         323.34,
         "Mice",
         2
        ],
        [
         "Redmond",
         "MSFT",
         "Shanghai",
         "distributer",
         true,
         2,
         323.34,
         "Keyboard",
         1
        ],
        [
         "Redmond",
         "MSFT",
         "Shanghai",
         "retail",
         true,
         1,
         123.34,
         "Laptop",
         20
        ],
        [
         "Redmond",
         "MSFT",
         "Shanghai",
         "retail",
         true,
         1,
         123.34,
         "Charger",
         2
        ],
        [
         "Redmond",
         "MSFT",
         "Shanghai",
         "retail",
         true,
         2,
         323.34,
         "Mice",
         2
        ],
        [
         "Redmond",
         "MSFT",
         "Shanghai",
         "retail",
         true,
         2,
         323.34,
         "Keyboard",
         1
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "location",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "satellites",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "goods_customers",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "goods_trade",
         "type": "\"boolean\""
        },
        {
         "metadata": "{}",
         "name": "goods_orders_orderId",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "goods_orders_orderTotal",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "goods_orders_shipped_orderItems_itemName",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "goods_orders_shipped_orderItems_itemQty",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_output = master_array(df)\n",
    "display(final_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6652b0e9-f912-43c1-ad6e-e14ebb416907",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Question 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "179446b0-1609-4e61-bc96-7d2b956a4c91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. count the number of words from given text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b391f8ab-0f7e-4d74-92ab-751115f695bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 2578 bytes.\nOut[84]: True"
     ]
    }
   ],
   "source": [
    "dbutils.fs.put(\"dbfs:/FileStore/workout_data/word_count.txt\", \"\"\" \n",
    "               RRR[note 1] (subtitled onscreen as Roudram Ranam Rudhiram) is a 2022 Indian Telugu-language epic period action drama film directed by S. S. Rajamouli, who co-wrote the film with V. Vijayendra Prasad. It was produced by D. V. V. Danayya under DVV Entertainment. The film stars N. T. Rama Rao Jr., Ram Charan, Ajay Devgn, Alia Bhatt, Shriya Saran, Samuthirakani, Ray Stevenson, Alison Doody, and Olivia Morris. It is a historical fiction film about two Indian revolutionaries, Alluri Sitarama Raju (Charan) and Komaram Bheem (Rama Rao), their friendship, and their fight against the British Raj.\n",
    "\n",
    "Made on a budget of 550 crore (US$74 million),[6] RRR was the most expensive Indian film at the time of its release. The film was released theatrically on 25 March 2022. With 223 crore (US$27 million) worldwide on its first day, RRR recorded the highest opening-day earned by an Indian film. It emerged as the highest-grossing film in its home market of Andhra Pradesh and Telangana, grossing over 405.9 crore (US$49 million).[7] The film grossed 1,389.31 crore (US$170 million) worldwide, setting several box office records for an Indian film, including the third highest-grossing Indian film, the second highest-grossing Telugu film, the highest grossing Telugu film of 2022 and the highest grossing Indian film of 2022 worldwide.[4][8]\n",
    "\n",
    "RRR received universal critical acclaim for its direction, screenwriting, cast performances, cinematography, soundtrack, action sequences and VFX. The film was considered one of the ten best films of the year by the National Board of Review, making it only the seventh non-English language film ever to make it to the list.[9] The song \"Naatu Naatu\" won the Oscar for Best Original Song at the 95th Academy Awards, making it the first song from an Indian film, as well as the first from an Asian film, to win in this category. The win made RRR the first Indian feature film to win an Academy Award.[10][11] The film became the third Indian film and first Telugu film to receive nominations at the Golden Globe Awards, including Best Foreign Language Film, and won Best Original Song for \"Naatu Naatu\", making it the first Indian (as well as the first Asian) nominee to win the award.[12][13] RRR also won the awards for Best Foreign Language Film and Best Song at the 28th Critics' Choice Awards. At the 69th National Film Awards, the film won six awards, including Best Popular Feature Film, Best Music Direction (Keeravani) and Best Male Playback Singer (Kaala Bhairava for \"Komuram Bheemudo\").\n",
    "\n",
    "\n",
    "               \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19983bc4-3a93-4659-9657-ceccdc7a4ffd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[90]: ['',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n 'rrr[note',\n '1]',\n '(subtitled',\n 'onscreen',\n 'as',\n 'roudram',\n 'ranam',\n 'rudhiram)',\n 'is',\n 'a',\n '2022',\n 'indian',\n 'telugu-language',\n 'epic',\n 'period',\n 'action',\n 'drama',\n 'film',\n 'directed',\n 'by',\n 's.',\n 's.',\n 'rajamouli,',\n 'who',\n 'co-wrote',\n 'the',\n 'film',\n 'with',\n 'v.',\n 'vijayendra',\n 'prasad.',\n 'it',\n 'was',\n 'produced',\n 'by',\n 'd.',\n 'v.',\n 'v.',\n 'danayya',\n 'under',\n 'dvv',\n 'entertainment.',\n 'the',\n 'film',\n 'stars',\n 'n.',\n 't.',\n 'rama',\n 'rao',\n 'jr.,',\n 'ram',\n 'charan,',\n 'ajay',\n 'devgn,',\n 'alia',\n 'bhatt,',\n 'shriya',\n 'saran,',\n 'samuthirakani,',\n 'ray',\n 'stevenson,',\n 'alison',\n 'doody,',\n 'and',\n 'olivia',\n 'morris.',\n 'it',\n 'is',\n 'a',\n 'historical',\n 'fiction',\n 'film',\n 'about',\n 'two',\n 'indian',\n 'revolutionaries,',\n 'alluri',\n 'sitarama',\n 'raju',\n '(charan)',\n 'and',\n 'komaram',\n 'bheem',\n '(rama',\n 'rao),',\n 'their',\n 'friendship,',\n 'and',\n 'their',\n 'fight',\n 'against',\n 'the',\n 'british',\n 'raj.',\n '',\n 'made',\n 'on',\n 'a',\n 'budget',\n 'of',\n '550',\n 'crore',\n '(us$74',\n 'million),[6]',\n 'rrr',\n 'was',\n 'the',\n 'most',\n 'expensive',\n 'indian',\n 'film',\n 'at',\n 'the',\n 'time',\n 'of',\n 'its',\n 'release.',\n 'the',\n 'film',\n 'was',\n 'released',\n 'theatrically',\n 'on',\n '25',\n 'march',\n '2022.',\n 'with',\n '223',\n 'crore',\n '(us$27',\n 'million)',\n 'worldwide',\n 'on',\n 'its',\n 'first',\n 'day,',\n 'rrr',\n 'recorded',\n 'the',\n 'highest',\n 'opening-day',\n 'earned',\n 'by',\n 'an',\n 'indian',\n 'film.',\n 'it',\n 'emerged',\n 'as',\n 'the',\n 'highest-grossing',\n 'film',\n 'in',\n 'its',\n 'home',\n 'market',\n 'of',\n 'andhra',\n 'pradesh',\n 'and',\n 'telangana,',\n 'grossing',\n 'over',\n '405.9',\n 'crore',\n '(us$49',\n 'million).[7]',\n 'the',\n 'film',\n 'grossed',\n '1,389.31',\n 'crore',\n '(us$170',\n 'million)',\n 'worldwide,',\n 'setting',\n 'several',\n 'box',\n 'office',\n 'records',\n 'for',\n 'an',\n 'indian',\n 'film,',\n 'including',\n 'the',\n 'third',\n 'highest-grossing',\n 'indian',\n 'film,',\n 'the',\n 'second',\n 'highest-grossing',\n 'telugu',\n 'film,',\n 'the',\n 'highest',\n 'grossing',\n 'telugu',\n 'film',\n 'of',\n '2022',\n 'and',\n 'the',\n 'highest',\n 'grossing',\n 'indian',\n 'film',\n 'of',\n '2022',\n 'worldwide.[4][8]',\n '',\n 'rrr',\n 'received',\n 'universal',\n 'critical',\n 'acclaim',\n 'for',\n 'its',\n 'direction,',\n 'screenwriting,',\n 'cast',\n 'performances,',\n 'cinematography,',\n 'soundtrack,',\n 'action',\n 'sequences',\n 'and',\n 'vfx.',\n 'the',\n 'film',\n 'was',\n 'considered',\n 'one',\n 'of',\n 'the',\n 'ten',\n 'best',\n 'films',\n 'of',\n 'the',\n 'year',\n 'by',\n 'the',\n 'national',\n 'board',\n 'of',\n 'review,',\n 'making',\n 'it',\n 'only',\n 'the',\n 'seventh',\n 'non-english',\n 'language',\n 'film',\n 'ever',\n 'to',\n 'make',\n 'it',\n 'to',\n 'the',\n 'list.[9]',\n 'the',\n 'song',\n '\"naatu',\n 'naatu\"',\n 'won',\n 'the',\n 'oscar',\n 'for',\n 'best',\n 'original',\n 'song',\n 'at',\n 'the',\n '95th',\n 'academy',\n 'awards,',\n 'making',\n 'it',\n 'the',\n 'first',\n 'song',\n 'from',\n 'an',\n 'indian',\n 'film,',\n 'as',\n 'well',\n 'as',\n 'the',\n 'first',\n 'from',\n 'an',\n 'asian',\n 'film,',\n 'to',\n 'win',\n 'in',\n 'this',\n 'category.',\n 'the',\n 'win',\n 'made',\n 'rrr',\n 'the',\n 'first',\n 'indian',\n 'feature',\n 'film',\n 'to',\n 'win',\n 'an',\n 'academy',\n 'award.[10][11]',\n 'the',\n 'film',\n 'became',\n 'the',\n 'third',\n 'indian',\n 'film',\n 'and',\n 'first',\n 'telugu',\n 'film',\n 'to',\n 'receive',\n 'nominations',\n 'at',\n 'the',\n 'golden',\n 'globe',\n 'awards,',\n 'including',\n 'best',\n 'foreign',\n 'language',\n 'film,',\n 'and',\n 'won',\n 'best',\n 'original',\n 'song',\n 'for',\n '\"naatu',\n 'naatu\",',\n 'making',\n 'it',\n 'the',\n 'first',\n 'indian',\n '(as',\n 'well',\n 'as',\n 'the',\n 'first',\n 'asian)',\n 'nominee',\n 'to',\n 'win',\n 'the',\n 'award.[12][13]',\n 'rrr',\n 'also',\n 'won',\n 'the',\n 'awards',\n 'for',\n 'best',\n 'foreign',\n 'language',\n 'film',\n 'and',\n 'best',\n 'song',\n 'at',\n 'the',\n '28th',\n \"critics'\",\n 'choice',\n 'awards.',\n 'at',\n 'the',\n '69th',\n 'national',\n 'film',\n 'awards,',\n 'the',\n 'film',\n 'won',\n 'six',\n 'awards,',\n 'including',\n 'best',\n 'popular',\n 'feature',\n 'film,',\n 'best',\n 'music',\n 'direction',\n '(keeravani)',\n 'and',\n 'best',\n 'male',\n 'playback',\n 'singer',\n '(kaala',\n 'bhairava',\n 'for',\n '\"komuram',\n 'bheemudo\").',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '']"
     ]
    }
   ],
   "source": [
    "RDD_1 = sc.textFile(\"dbfs:/FileStore/workout_data/word_count.txt\")\n",
    "RDD_2 =RDD_1.flatMap(lambda x: x.lower().split(\" \"))\n",
    "RDD_2.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c080b93-35a7-4b21-8284-1ec9783bf389",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[96]: [(1, '1]'),\n (1, '(subtitled'),\n (1, 'onscreen'),\n (1, 'roudram'),\n (1, 'rudhiram)'),\n (1, 'epic'),\n (1, 'period'),\n (1, 'drama'),\n (1, 'co-wrote'),\n (1, 'prasad.'),\n (1, 'produced'),\n (1, 'd.'),\n (1, 'n.'),\n (1, 'rama'),\n (1, 'jr.,'),\n (1, 'shriya'),\n (1, 'saran,'),\n (1, 'samuthirakani,'),\n (1, 'alison'),\n (1, 'doody,'),\n (1, 'olivia'),\n (1, 'morris.'),\n (1, 'two'),\n (1, 'revolutionaries,'),\n (1, '(charan)'),\n (1, 'bheem'),\n (1, 'friendship,'),\n (1, 'fight'),\n (1, 'against'),\n (1, '(us$74'),\n (1, 'million),[6]'),\n (1, 'release.'),\n (1, 'opening-day'),\n (1, 'earned'),\n (1, 'home'),\n (1, 'andhra'),\n (1, 'pradesh'),\n (1, 'telangana,'),\n (1, 'million).[7]'),\n (1, 'grossed'),\n (1, 'several'),\n (1, 'universal'),\n (1, 'direction,'),\n (1, 'screenwriting,'),\n (1, 'cast'),\n (1, 'sequences'),\n (1, 'considered'),\n (1, 'ten'),\n (1, 'films'),\n (1, 'year'),\n (1, 'board'),\n (1, 'only'),\n (1, 'seventh'),\n (1, 'ever'),\n (1, 'make'),\n (1, 'list.[9]'),\n (1, 'oscar'),\n (1, '95th'),\n (1, 'asian'),\n (1, 'this'),\n (1, 'category.'),\n (1, 'naatu\",'),\n (1, '(as'),\n (1, 'asian)'),\n (1, 'awards'),\n (1, 'awards.'),\n (1, '69th'),\n (1, 'music'),\n (1, 'direction'),\n (1, '(keeravani)'),\n (1, 'playback'),\n (1, 'singer'),\n (1, '(kaala'),\n (1, 'bhairava'),\n (1, '\"komuram'),\n (1, 'bheemudo\").'),\n (1, 'rrr[note'),\n (1, 'ranam'),\n (1, 'telugu-language'),\n (1, 'directed'),\n (1, 'rajamouli,'),\n (1, 'who'),\n (1, 'vijayendra'),\n (1, 'danayya'),\n (1, 'under'),\n (1, 'dvv'),\n (1, 'entertainment.'),\n (1, 'stars'),\n (1, 't.'),\n (1, 'rao'),\n (1, 'ram'),\n (1, 'charan,'),\n (1, 'ajay'),\n (1, 'devgn,'),\n (1, 'alia'),\n (1, 'bhatt,'),\n (1, 'ray'),\n (1, 'stevenson,'),\n (1, 'historical'),\n (1, 'fiction'),\n (1, 'about'),\n (1, 'alluri'),\n (1, 'sitarama'),\n (1, 'raju'),\n (1, 'komaram'),\n (1, '(rama'),\n (1, 'rao),'),\n (1, 'british'),\n (1, 'raj.'),\n (1, 'budget'),\n (1, '550'),\n (1, 'most'),\n (1, 'expensive'),\n (1, 'time'),\n (1, 'released'),\n (1, 'theatrically'),\n (1, '25'),\n (1, 'march'),\n (1, '2022.'),\n (1, '223'),\n (1, '(us$27'),\n (1, 'worldwide'),\n (1, 'day,'),\n (1, 'recorded'),\n (1, 'film.'),\n (1, 'emerged'),\n (1, 'market'),\n (1, 'over'),\n (1, '405.9'),\n (1, '(us$49'),\n (1, '1,389.31'),\n (1, '(us$170'),\n (1, 'worldwide,'),\n (1, 'setting'),\n (1, 'box'),\n (1, 'office'),\n (1, 'records'),\n (1, 'second'),\n (1, 'worldwide.[4][8]'),\n (1, 'received'),\n (1, 'critical'),\n (1, 'acclaim'),\n (1, 'performances,'),\n (1, 'cinematography,'),\n (1, 'soundtrack,'),\n (1, 'vfx.'),\n (1, 'one'),\n (1, 'review,'),\n (1, 'non-english'),\n (1, 'naatu\"'),\n (1, 'award.[10][11]'),\n (1, 'became'),\n (1, 'receive'),\n (1, 'nominations'),\n (1, 'golden'),\n (1, 'globe'),\n (1, 'nominee'),\n (1, 'award.[12][13]'),\n (1, 'also'),\n (1, '28th'),\n (1, \"critics'\"),\n (1, 'choice'),\n (1, 'six'),\n (1, 'popular'),\n (1, 'male'),\n (2, 'is'),\n (2, 'action'),\n (2, 'million)'),\n (2, 'in'),\n (2, 'third'),\n (2, '\"naatu'),\n (2, 'feature'),\n (2, 'foreign'),\n (2, 's.'),\n (2, 'with'),\n (2, 'their'),\n (2, 'made'),\n (2, 'national'),\n (2, 'original'),\n (2, 'academy'),\n (2, 'from'),\n (2, 'well'),\n (3, 'v.'),\n (3, 'telugu'),\n (3, 'making'),\n (3, 'language'),\n (3, 'a'),\n (3, '2022'),\n (3, 'on'),\n (3, 'highest'),\n (3, 'highest-grossing'),\n (3, 'grossing'),\n (3, 'including'),\n (4, 'was'),\n (4, 'crore'),\n (4, 'win'),\n (4, 'by'),\n (4, 'its'),\n (4, 'won'),\n (4, 'awards,'),\n (5, 'as'),\n (5, 'rrr'),\n (5, 'at'),\n (5, 'an'),\n (5, 'song'),\n (6, 'for'),\n (6, 'to'),\n (7, 'it'),\n (7, 'first'),\n (7, 'film,'),\n (8, 'of'),\n (9, 'best'),\n (10, 'and'),\n (11, 'indian'),\n (19, 'film'),\n (36, 'the')]"
     ]
    }
   ],
   "source": [
    "RDD_3 = RDD_2.filter(lambda a: a!= '')\n",
    "RDD_3.map(lambda a: (a,1)).reduceByKey(lambda a,b : a+b).map(lambda a: (a[1], a[0])).sortByKey().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "768fabfd-41b7-4773-8c50-02438708d386",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Question 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19a2dc30-33ce-4094-82b5-acafa087dea9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Handling of currupted or bad record data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca4550e0-ed32-465f-b0f5-fabf61020ddd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 471 bytes.\nOut[104]: True"
     ]
    }
   ],
   "source": [
    "dbutils.fs.put(\"dbfs:/FileStore/workout_data/bad_record.csv\", \"\"\"CHANNEL_ID,CHANNEL_DESC,CHANNEL_CLASS,CHANNEL_CLASS_ID,CHANNEL_TOTAL,CHANNEL_TOTAL_ID\n",
    "3,Direct Sales,Direct,12,Channel total,1\n",
    "9,Tele Sales,Direct,12,Channel total,1\n",
    "5,Catalog,Indirect,13,Channel total,1\n",
    "4,Internet,Indirect,13,Channel total,1\n",
    "2,Partners,Others,14,Channel total,1\n",
    "12,Partners,Others,14,Channel total,1,45,ram,343\n",
    "alpa,Partners,Others,14,Channel total,1,45,ram,343\n",
    "10 Partners Others 14 Channel total 1\n",
    "11 Partners Others 14 Channel total 1\n",
    "               \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e897cb16-bd9f-4235-833e-cbb1c72c5fec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method csv in module pyspark.sql.readwriter:\n\ncsv(path: Union[str, List[str]], schema: Union[pyspark.sql.types.StructType, str, NoneType] = None, sep: Optional[str] = None, encoding: Optional[str] = None, quote: Optional[str] = None, escape: Optional[str] = None, comment: Optional[str] = None, header: Union[bool, str, NoneType] = None, inferSchema: Union[bool, str, NoneType] = None, ignoreLeadingWhiteSpace: Union[bool, str, NoneType] = None, ignoreTrailingWhiteSpace: Union[bool, str, NoneType] = None, nullValue: Optional[str] = None, nanValue: Optional[str] = None, positiveInf: Optional[str] = None, negativeInf: Optional[str] = None, dateFormat: Optional[str] = None, timestampFormat: Optional[str] = None, maxColumns: Union[int, str, NoneType] = None, maxCharsPerColumn: Union[int, str, NoneType] = None, maxMalformedLogPerPartition: Union[int, str, NoneType] = None, mode: Optional[str] = None, columnNameOfCorruptRecord: Optional[str] = None, multiLine: Union[bool, str, NoneType] = None, charToEscapeQuoteEscaping: Optional[str] = None, samplingRatio: Union[str, float, NoneType] = None, enforceSchema: Union[bool, str, NoneType] = None, emptyValue: Optional[str] = None, locale: Optional[str] = None, lineSep: Optional[str] = None, pathGlobFilter: Union[bool, str, NoneType] = None, recursiveFileLookup: Union[bool, str, NoneType] = None, modifiedBefore: Union[bool, str, NoneType] = None, modifiedAfter: Union[bool, str, NoneType] = None, unescapedQuoteHandling: Optional[str] = None) -> 'DataFrame' method of pyspark.sql.readwriter.DataFrameReader instance\n    Loads a CSV file and returns the result as a  :class:`DataFrame`.\n    \n    This function will go through the input once to determine the input schema if\n    ``inferSchema`` is enabled. To avoid going through the entire data once, disable\n    ``inferSchema`` option or specify the schema explicitly using ``schema``.\n    \n    .. versionadded:: 2.0.0\n    \n    .. versionchanged:: 3.4.0\n        Support Spark Connect.\n    \n    Parameters\n    ----------\n    path : str or list\n        string, or list of strings, for input path(s),\n        or RDD of Strings storing CSV rows.\n    schema : :class:`pyspark.sql.types.StructType` or str, optional\n        an optional :class:`pyspark.sql.types.StructType` for the input schema\n        or a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).\n    \n    Other Parameters\n    ----------------\n    Extra options\n        For the extra options, refer to\n        `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-csv.html#data-source-option>`_\n        for the version you use.\n    \n        .. # noqa\n    \n    Examples\n    --------\n    Write a DataFrame into a CSV file and read it back.\n    \n    >>> import tempfile\n    >>> with tempfile.TemporaryDirectory() as d:\n    ...     # Write a DataFrame into a CSV file\n    ...     df = spark.createDataFrame([{\"age\": 100, \"name\": \"Hyukjin Kwon\"}])\n    ...     df.write.mode(\"overwrite\").format(\"csv\").save(d)\n    ...\n    ...     # Read the CSV file as a DataFrame with 'nullValue' option set to 'Hyukjin Kwon'.\n    ...     spark.read.csv(d, schema=df.schema, nullValue=\"Hyukjin Kwon\").show()\n    +---+----+\n    |age|name|\n    +---+----+\n    |100|null|\n    +---+----+\n\n"
     ]
    }
   ],
   "source": [
    "help(spark.read.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe3c4559-481f-4cf3-8138-5c16530e5ffa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[4]: StructType([StructField('CHANNEL_ID', IntegerType(), True), StructField('CHANNEL_DESC', StringType(), True), StructField('CHANNEL_CLASS', StringType(), True), StructField('CHANNEL_CLASS_ID', IntegerType(), True), StructField('CHANNEL_TOTAL', StringType(), True), StructField('CHANNEL_TOTAL_ID', IntegerType(), True)])"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Define the schema\n",
    "df_schema = StructType([\n",
    "    StructField('CHANNEL_ID', IntegerType(), True), \n",
    "    StructField('CHANNEL_DESC', StringType(), True), \n",
    "    StructField('CHANNEL_CLASS', StringType(), True), \n",
    "    StructField('CHANNEL_CLASS_ID', IntegerType(), True), \n",
    "    StructField('CHANNEL_TOTAL', StringType(), True), \n",
    "    StructField('CHANNEL_TOTAL_ID', IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Read the CSV file with the specified schema\n",
    "df = spark.read.schema(df_schema) \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"badRecordsPath\", \"dbfs:/FileStore/workout_data/bad_record_log/\") \\\n",
    "    .csv(\"dbfs:/FileStore/workout_data/bad_record.csv\")\n",
    "\n",
    "# Display the schema of the DataFrame\n",
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5534456f-cd38-49b1-bde9-d6137b2561dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>CHANNEL_ID</th><th>CHANNEL_DESC</th><th>CHANNEL_CLASS</th><th>CHANNEL_CLASS_ID</th><th>CHANNEL_TOTAL</th><th>CHANNEL_TOTAL_ID</th></tr></thead><tbody><tr><td>3</td><td>Direct Sales</td><td>Direct</td><td>12</td><td>Channel total</td><td>1</td></tr><tr><td>9</td><td>Tele Sales</td><td>Direct</td><td>12</td><td>Channel total</td><td>1</td></tr><tr><td>5</td><td>Catalog</td><td>Indirect</td><td>13</td><td>Channel total</td><td>1</td></tr><tr><td>4</td><td>Internet</td><td>Indirect</td><td>13</td><td>Channel total</td><td>1</td></tr><tr><td>2</td><td>Partners</td><td>Others</td><td>14</td><td>Channel total</td><td>1</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         3,
         "Direct Sales",
         "Direct",
         12,
         "Channel total",
         1
        ],
        [
         9,
         "Tele Sales",
         "Direct",
         12,
         "Channel total",
         1
        ],
        [
         5,
         "Catalog",
         "Indirect",
         13,
         "Channel total",
         1
        ],
        [
         4,
         "Internet",
         "Indirect",
         13,
         "Channel total",
         1
        ],
        [
         2,
         "Partners",
         "Others",
         14,
         "Channel total",
         1
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "CHANNEL_ID",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "CHANNEL_DESC",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "CHANNEL_CLASS",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "CHANNEL_CLASS_ID",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "CHANNEL_TOTAL",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "CHANNEL_TOTAL_ID",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b59a13e-d346-4e67-8df4-2a8b16956671",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>_c0</th><th>_c1</th><th>_c2</th><th>_c3</th><th>_c4</th><th>_c5</th><th>_c6</th><th>_c7</th><th>_c8</th><th>_c9</th><th>_c10</th><th>_c11</th><th>_c12</th><th>_c13</th><th>_c14</th><th>_c15</th><th>_c16</th><th>_c17</th><th>_c18</th></tr></thead><tbody><tr><td>{\"path\":\"dbfs:/FileStore/workout_data/bad_record.csv\"</td><td>\"record\":\"12</td><td>Partners</td><td>Others</td><td>14</td><td>Channel total</td><td>1</td><td>45</td><td>ram</td><td>343\"</td><td>\"reason\":\"org.apache.spark.SparkRuntimeException: [MALFORMED_CSV_RECORD] Malformed CSV record: 12</td><td>Partners</td><td>Others</td><td>14</td><td>Channel total</td><td>1</td><td>45</td><td>ram</td><td>343\"}</td></tr><tr><td>{\"path\":\"dbfs:/FileStore/workout_data/bad_record.csv\"</td><td>\"record\":\"alpa</td><td>Partners</td><td>Others</td><td>14</td><td>Channel total</td><td>1</td><td>45</td><td>ram</td><td>343\"</td><td>\"reason\":\"org.apache.spark.SparkRuntimeException: [MALFORMED_CSV_RECORD] Malformed CSV record: alpa</td><td>Partners</td><td>Others</td><td>14</td><td>Channel total</td><td>1</td><td>45</td><td>ram</td><td>343\"}</td></tr><tr><td>{\"path\":\"dbfs:/FileStore/workout_data/bad_record.csv\"</td><td>\"record\":\"10 Partners Others 14 Channel total 1\"</td><td>\"reason\":\"org.apache.spark.SparkRuntimeException: [MALFORMED_CSV_RECORD] Malformed CSV record: 10 Partners Others 14 Channel total 1\"}</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>{\"path\":\"dbfs:/FileStore/workout_data/bad_record.csv\"</td><td>\"record\":\"11 Partners Others 14 Channel total 1\"</td><td>\"reason\":\"org.apache.spark.SparkRuntimeException: [MALFORMED_CSV_RECORD] Malformed CSV record: 11 Partners Others 14 Channel total 1\"}</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "{\"path\":\"dbfs:/FileStore/workout_data/bad_record.csv\"",
         "\"record\":\"12",
         "Partners",
         "Others",
         "14",
         "Channel total",
         "1",
         "45",
         "ram",
         "343\"",
         "\"reason\":\"org.apache.spark.SparkRuntimeException: [MALFORMED_CSV_RECORD] Malformed CSV record: 12",
         "Partners",
         "Others",
         "14",
         "Channel total",
         "1",
         "45",
         "ram",
         "343\"}"
        ],
        [
         "{\"path\":\"dbfs:/FileStore/workout_data/bad_record.csv\"",
         "\"record\":\"alpa",
         "Partners",
         "Others",
         "14",
         "Channel total",
         "1",
         "45",
         "ram",
         "343\"",
         "\"reason\":\"org.apache.spark.SparkRuntimeException: [MALFORMED_CSV_RECORD] Malformed CSV record: alpa",
         "Partners",
         "Others",
         "14",
         "Channel total",
         "1",
         "45",
         "ram",
         "343\"}"
        ],
        [
         "{\"path\":\"dbfs:/FileStore/workout_data/bad_record.csv\"",
         "\"record\":\"10 Partners Others 14 Channel total 1\"",
         "\"reason\":\"org.apache.spark.SparkRuntimeException: [MALFORMED_CSV_RECORD] Malformed CSV record: 10 Partners Others 14 Channel total 1\"}",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "{\"path\":\"dbfs:/FileStore/workout_data/bad_record.csv\"",
         "\"record\":\"11 Partners Others 14 Channel total 1\"",
         "\"reason\":\"org.apache.spark.SparkRuntimeException: [MALFORMED_CSV_RECORD] Malformed CSV record: 11 Partners Others 14 Channel total 1\"}",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "_c0",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "_c1",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "_c2",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "_c3",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "_c4",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "_c5",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "_c6",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "_c7",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "_c8",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "_c9",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "_c10",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "_c11",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "_c12",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "_c13",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "_c14",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "_c15",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "_c16",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "_c17",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "_c18",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(spark.read.csv(\"dbfs:/FileStore/workout_data/bad_record_log/20240823T064339/bad_records/part-00000-6446d0b3-45c6-429a-9f29-f7b2e9a58651\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07e62305-c730-4b69-b607-536342d8e42f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Question 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c04acf5-3b89-4ead-b6df-299cb7de73e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Flatten the Nested JSON "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1dcf113-b103-4d1d-9d2e-772b34837f0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>batters</th><th>id</th><th>name</th><th>ppu</th><th>topping</th><th>type</th></tr></thead><tbody><tr><td>List(List(List(1001, Regular), List(1002, Chocolate), List(1003, Blueberry), List(1004, Devil's Food)))</td><td>0001</td><td>Cake</td><td>0.55</td><td>List(List(5001, None), List(5002, Glazed), List(5005, Sugar), List(5007, Powdered Sugar), List(5006, Chocolate with Sprinkles), List(5003, Chocolate), List(5004, Maple))</td><td>donut</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         [
          [
           [
            "1001",
            "Regular"
           ],
           [
            "1002",
            "Chocolate"
           ],
           [
            "1003",
            "Blueberry"
           ],
           [
            "1004",
            "Devil's Food"
           ]
          ]
         ],
         "0001",
         "Cake",
         0.55,
         [
          [
           "5001",
           "None"
          ],
          [
           "5002",
           "Glazed"
          ],
          [
           "5005",
           "Sugar"
          ],
          [
           "5007",
           "Powdered Sugar"
          ],
          [
           "5006",
           "Chocolate with Sprinkles"
          ],
          [
           "5003",
           "Chocolate"
          ],
          [
           "5004",
           "Maple"
          ]
         ],
         "donut"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "batters",
         "type": "{\"type\":\"struct\",\"fields\":[{\"name\":\"batter\",\"type\":{\"type\":\"array\",\"elementType\":{\"type\":\"struct\",\"fields\":[{\"name\":\"id\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"type\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]},\"containsNull\":true},\"nullable\":true,\"metadata\":{}}]}"
        },
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "ppu",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "topping",
         "type": "{\"type\":\"array\",\"elementType\":{\"type\":\"struct\",\"fields\":[{\"name\":\"id\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"type\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]},\"containsNull\":true}"
        },
        {
         "metadata": "{}",
         "name": "type",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- batters: struct (nullable = true)\n |    |-- batter: array (nullable = true)\n |    |    |-- element: struct (containsNull = true)\n |    |    |    |-- id: string (nullable = true)\n |    |    |    |-- type: string (nullable = true)\n |-- id: string (nullable = true)\n |-- name: string (nullable = true)\n |-- ppu: double (nullable = true)\n |-- topping: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- id: string (nullable = true)\n |    |    |-- type: string (nullable = true)\n |-- type: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df=spark.read.option(\"multiLine\", \"true\").json(\"dbfs:/FileStore/json_example\")\n",
    "display(df)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8ada694-a2ef-4599-b6ab-eaee40f69e3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "df = df.withColumn(\"batters\", explode(\"batters.batter\")) \\\n",
    "    .withColumn(\"topping\", explode(\"topping\")) \\\n",
    "    .select(\"id\", \"name\", \"ppu\", \"batters.id\", \"batters.type\", \"topping.id\", \"topping.type\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83306acd-624a-4658-bc54-1bf88c07aa5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>name</th><th>ppu</th><th>id</th><th>type</th><th>id</th><th>type</th></tr></thead><tbody><tr><td>0001</td><td>Cake</td><td>0.55</td><td>1001</td><td>Regular</td><td>5001</td><td>None</td></tr><tr><td>0001</td><td>Cake</td><td>0.55</td><td>1001</td><td>Regular</td><td>5002</td><td>Glazed</td></tr><tr><td>0001</td><td>Cake</td><td>0.55</td><td>1001</td><td>Regular</td><td>5005</td><td>Sugar</td></tr><tr><td>0001</td><td>Cake</td><td>0.55</td><td>1001</td><td>Regular</td><td>5007</td><td>Powdered Sugar</td></tr><tr><td>0001</td><td>Cake</td><td>0.55</td><td>1001</td><td>Regular</td><td>5006</td><td>Chocolate with Sprinkles</td></tr><tr><td>0001</td><td>Cake</td><td>0.55</td><td>1001</td><td>Regular</td><td>5003</td><td>Chocolate</td></tr><tr><td>0001</td><td>Cake</td><td>0.55</td><td>1001</td><td>Regular</td><td>5004</td><td>Maple</td></tr><tr><td>0001</td><td>Cake</td><td>0.55</td><td>1002</td><td>Chocolate</td><td>5001</td><td>None</td></tr><tr><td>0001</td><td>Cake</td><td>0.55</td><td>1002</td><td>Chocolate</td><td>5002</td><td>Glazed</td></tr><tr><td>0001</td><td>Cake</td><td>0.55</td><td>1002</td><td>Chocolate</td><td>5005</td><td>Sugar</td></tr><tr><td>0001</td><td>Cake</td><td>0.55</td><td>1002</td><td>Chocolate</td><td>5007</td><td>Powdered Sugar</td></tr><tr><td>0001</td><td>Cake</td><td>0.55</td><td>1002</td><td>Chocolate</td><td>5006</td><td>Chocolate with Sprinkles</td></tr><tr><td>0001</td><td>Cake</td><td>0.55</td><td>1002</td><td>Chocolate</td><td>5003</td><td>Chocolate</td></tr><tr><td>0001</td><td>Cake</td><td>0.55</td><td>1002</td><td>Chocolate</td><td>5004</td><td>Maple</td></tr><tr><td>0001</td><td>Cake</td><td>0.55</td><td>1003</td><td>Blueberry</td><td>5001</td><td>None</td></tr><tr><td>0001</td><td>Cake</td><td>0.55</td><td>1003</td><td>Blueberry</td><td>5002</td><td>Glazed</td></tr><tr><td>0001</td><td>Cake</td><td>0.55</td><td>1003</td><td>Blueberry</td><td>5005</td><td>Sugar</td></tr><tr><td>0001</td><td>Cake</td><td>0.55</td><td>1003</td><td>Blueberry</td><td>5007</td><td>Powdered Sugar</td></tr><tr><td>0001</td><td>Cake</td><td>0.55</td><td>1003</td><td>Blueberry</td><td>5006</td><td>Chocolate with Sprinkles</td></tr><tr><td>0001</td><td>Cake</td><td>0.55</td><td>1003</td><td>Blueberry</td><td>5003</td><td>Chocolate</td></tr><tr><td>0001</td><td>Cake</td><td>0.55</td><td>1003</td><td>Blueberry</td><td>5004</td><td>Maple</td></tr><tr><td>0001</td><td>Cake</td><td>0.55</td><td>1004</td><td>Devil's Food</td><td>5001</td><td>None</td></tr><tr><td>0001</td><td>Cake</td><td>0.55</td><td>1004</td><td>Devil's Food</td><td>5002</td><td>Glazed</td></tr><tr><td>0001</td><td>Cake</td><td>0.55</td><td>1004</td><td>Devil's Food</td><td>5005</td><td>Sugar</td></tr><tr><td>0001</td><td>Cake</td><td>0.55</td><td>1004</td><td>Devil's Food</td><td>5007</td><td>Powdered Sugar</td></tr><tr><td>0001</td><td>Cake</td><td>0.55</td><td>1004</td><td>Devil's Food</td><td>5006</td><td>Chocolate with Sprinkles</td></tr><tr><td>0001</td><td>Cake</td><td>0.55</td><td>1004</td><td>Devil's Food</td><td>5003</td><td>Chocolate</td></tr><tr><td>0001</td><td>Cake</td><td>0.55</td><td>1004</td><td>Devil's Food</td><td>5004</td><td>Maple</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "0001",
         "Cake",
         0.55,
         "1001",
         "Regular",
         "5001",
         "None"
        ],
        [
         "0001",
         "Cake",
         0.55,
         "1001",
         "Regular",
         "5002",
         "Glazed"
        ],
        [
         "0001",
         "Cake",
         0.55,
         "1001",
         "Regular",
         "5005",
         "Sugar"
        ],
        [
         "0001",
         "Cake",
         0.55,
         "1001",
         "Regular",
         "5007",
         "Powdered Sugar"
        ],
        [
         "0001",
         "Cake",
         0.55,
         "1001",
         "Regular",
         "5006",
         "Chocolate with Sprinkles"
        ],
        [
         "0001",
         "Cake",
         0.55,
         "1001",
         "Regular",
         "5003",
         "Chocolate"
        ],
        [
         "0001",
         "Cake",
         0.55,
         "1001",
         "Regular",
         "5004",
         "Maple"
        ],
        [
         "0001",
         "Cake",
         0.55,
         "1002",
         "Chocolate",
         "5001",
         "None"
        ],
        [
         "0001",
         "Cake",
         0.55,
         "1002",
         "Chocolate",
         "5002",
         "Glazed"
        ],
        [
         "0001",
         "Cake",
         0.55,
         "1002",
         "Chocolate",
         "5005",
         "Sugar"
        ],
        [
         "0001",
         "Cake",
         0.55,
         "1002",
         "Chocolate",
         "5007",
         "Powdered Sugar"
        ],
        [
         "0001",
         "Cake",
         0.55,
         "1002",
         "Chocolate",
         "5006",
         "Chocolate with Sprinkles"
        ],
        [
         "0001",
         "Cake",
         0.55,
         "1002",
         "Chocolate",
         "5003",
         "Chocolate"
        ],
        [
         "0001",
         "Cake",
         0.55,
         "1002",
         "Chocolate",
         "5004",
         "Maple"
        ],
        [
         "0001",
         "Cake",
         0.55,
         "1003",
         "Blueberry",
         "5001",
         "None"
        ],
        [
         "0001",
         "Cake",
         0.55,
         "1003",
         "Blueberry",
         "5002",
         "Glazed"
        ],
        [
         "0001",
         "Cake",
         0.55,
         "1003",
         "Blueberry",
         "5005",
         "Sugar"
        ],
        [
         "0001",
         "Cake",
         0.55,
         "1003",
         "Blueberry",
         "5007",
         "Powdered Sugar"
        ],
        [
         "0001",
         "Cake",
         0.55,
         "1003",
         "Blueberry",
         "5006",
         "Chocolate with Sprinkles"
        ],
        [
         "0001",
         "Cake",
         0.55,
         "1003",
         "Blueberry",
         "5003",
         "Chocolate"
        ],
        [
         "0001",
         "Cake",
         0.55,
         "1003",
         "Blueberry",
         "5004",
         "Maple"
        ],
        [
         "0001",
         "Cake",
         0.55,
         "1004",
         "Devil's Food",
         "5001",
         "None"
        ],
        [
         "0001",
         "Cake",
         0.55,
         "1004",
         "Devil's Food",
         "5002",
         "Glazed"
        ],
        [
         "0001",
         "Cake",
         0.55,
         "1004",
         "Devil's Food",
         "5005",
         "Sugar"
        ],
        [
         "0001",
         "Cake",
         0.55,
         "1004",
         "Devil's Food",
         "5007",
         "Powdered Sugar"
        ],
        [
         "0001",
         "Cake",
         0.55,
         "1004",
         "Devil's Food",
         "5006",
         "Chocolate with Sprinkles"
        ],
        [
         "0001",
         "Cake",
         0.55,
         "1004",
         "Devil's Food",
         "5003",
         "Chocolate"
        ],
        [
         "0001",
         "Cake",
         0.55,
         "1004",
         "Devil's Food",
         "5004",
         "Maple"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "ppu",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "type",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "type",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3615c508-d08a-4c46-8fbe-063c0f673ea2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Question 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84207fe7-6362-4644-8525-e3000305cdb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. How would you calculate the monthly percentage change in revenue using PySpark, ensuring that the output displays only the \"year-month\" (ym) and revenue_diff_pct columns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a87db63a-2bda-4c49-9328-82f6a61b05c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------+-----------+\n| id|created_at| value|purchase_id|\n+---+----------+------+-----------+\n|  1|2019-01-01|172692|         43|\n|  2|2019-01-05|177194|         36|\n|  3|2019-01-09|109513|         30|\n|  4|2019-01-13|164911|         30|\n|  5|2019-01-17|198872|         39|\n|  6|2019-01-21|184853|         31|\n|  7|2019-01-25|186817|         26|\n|  8|2019-01-29|137784|         22|\n|  9|2019-02-02|140032|         25|\n| 10|2019-02-06|116948|         43|\n| 11|2019-02-10|162515|         25|\n+---+----------+------+-----------+\n\n+---+----------+------+-----------+-------+---------+\n| id|created_at| value|purchase_id|     ym|lag_value|\n+---+----------+------+-----------+-------+---------+\n|  1|2019-01-01|172692|         43|2019-01|     null|\n|  2|2019-01-05|177194|         36|2019-01|   172692|\n|  3|2019-01-09|109513|         30|2019-01|   177194|\n|  4|2019-01-13|164911|         30|2019-01|   109513|\n|  5|2019-01-17|198872|         39|2019-01|   164911|\n|  6|2019-01-21|184853|         31|2019-01|   198872|\n|  7|2019-01-25|186817|         26|2019-01|   184853|\n|  8|2019-01-29|137784|         22|2019-01|   186817|\n|  9|2019-02-02|140032|         25|2019-02|     null|\n| 10|2019-02-06|116948|         43|2019-02|   140032|\n| 11|2019-02-10|162515|         25|2019-02|   116948|\n+---+----------+------+-----------+-------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lag, col, date_format\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"SampleDataFrame\").getOrCreate()\n",
    "\n",
    "# Create the sample data\n",
    "data = [\n",
    "    (1, \"2019-01-01\", 172692, 43),\n",
    "    (2, \"2019-01-05\", 177194, 36),\n",
    "    (3, \"2019-01-09\", 109513, 30),\n",
    "    (4, \"2019-01-13\", 164911, 30),\n",
    "    (5, \"2019-01-17\", 198872, 39),\n",
    "    (6, \"2019-01-21\", 184853, 31),\n",
    "    (7, \"2019-01-25\", 186817, 26),\n",
    "    (8, \"2019-01-29\", 137784, 22),\n",
    "    (9, \"2019-02-02\", 140032, 25),\n",
    "    (10, \"2019-02-06\", 116948, 43),\n",
    "    (11, \"2019-02-10\", 162515, 25)\n",
    "]\n",
    "\n",
    "# Define the schema for the DataFrame\n",
    "columns = [\"id\", \"created_at\", \"value\", \"purchase_id\"]\n",
    "\n",
    "# Create a PySpark DataFrame\n",
    "df = spark.createDataFrame(data, schema=columns)\n",
    "\n",
    "df.show()\n",
    "\n",
    "df.createOrReplaceTempView('df')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f76108d-1351-470b-8fae-79200467186b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+\n|     ym|revenue_diff_pct|\n+-------+----------------+\n|2019-01|            null|\n|2019-01|            2.61|\n|2019-01|           -38.2|\n|2019-01|           50.59|\n|2019-01|           20.59|\n|2019-01|           -7.05|\n|2019-01|            1.06|\n|2019-01|          -26.25|\n|2019-02|            null|\n|2019-02|          -16.48|\n|2019-02|           38.96|\n+-------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, round\n",
    "\n",
    "# Convert 'created_at' to date format and extract year and month\n",
    "df = df.withColumn(\"created_at\", col(\"created_at\").cast(\"date\"))\n",
    "df = df.withColumn(\"ym\", date_format(col(\"created_at\"), \"yyyy-MM\"))\n",
    "\n",
    "# Define the window specification\n",
    "window_spec = Window.partitionBy(\"ym\").orderBy(\"ym\")\n",
    "\n",
    "# Apply the lag function over the defined window\n",
    "df = df.withColumn(\"lag_value\", lag(col(\"value\")).over(window_spec))\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "df.show()\n",
    "# Perform the calculation and aggregation\n",
    "df = df.withColumn(\n",
    "    \"revenue_diff_pct\",\n",
    "    round(((col(\"value\") - col(\"lag_value\")) / col(\"lag_value\")) * 100, 2)\n",
    ").select(\"ym\", \"revenue_diff_pct\")\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ebd7f444-ed53-4c9c-9060-d4c0aab29b4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Question 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67b2e969-c15c-4ade-9cbf-c2e95293c014",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>ym</th><th>revenue_diff_pct</th></tr></thead><tbody><tr><td>2019-01</td><td>null</td></tr><tr><td>2019-01</td><td>2.61</td></tr><tr><td>2019-01</td><td>-38.2</td></tr><tr><td>2019-01</td><td>50.59</td></tr><tr><td>2019-01</td><td>20.59</td></tr><tr><td>2019-01</td><td>-7.05</td></tr><tr><td>2019-01</td><td>1.06</td></tr><tr><td>2019-01</td><td>-26.25</td></tr><tr><td>2019-02</td><td>null</td></tr><tr><td>2019-02</td><td>-16.48</td></tr><tr><td>2019-02</td><td>38.96</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "2019-01",
         null
        ],
        [
         "2019-01",
         2.61
        ],
        [
         "2019-01",
         -38.2
        ],
        [
         "2019-01",
         50.59
        ],
        [
         "2019-01",
         20.59
        ],
        [
         "2019-01",
         -7.05
        ],
        [
         "2019-01",
         1.06
        ],
        [
         "2019-01",
         -26.25
        ],
        [
         "2019-02",
         null
        ],
        [
         "2019-02",
         -16.48
        ],
        [
         "2019-02",
         38.96
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "ym",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "revenue_diff_pct",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "WITH temp AS (\n",
    "    SELECT\n",
    "        ym,\n",
    "        value,\n",
    "        LAG(value) OVER (PARTITION BY ym ORDER BY ym) AS lag_value\n",
    "    FROM (\n",
    "        SELECT\n",
    "            date_format(created_at, 'yyyy-MM') AS ym,\n",
    "            value\n",
    "        FROM df\n",
    "    )\n",
    ")\n",
    "\n",
    "-- Step 3: Calculate the percentage difference\n",
    "SELECT\n",
    "    ym,\n",
    "    ROUND(((value - lag_value) / lag_value) * 100, 2) AS revenue_diff_pct\n",
    "FROM temp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6f06030-f7c6-4561-b446-fa9f85c8bd30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Question 17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d89433fa-d0dc-4f26-8bf3-b30397ef5f12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Find the senior most employee categorised based on designation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea077dad-52b7-4f89-87e8-a94855bb7b07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+--------------+-----------------+---+\n|employee_id|name|effective_date|      designation|rnk|\n+-----------+----+--------------+-----------------+---+\n|          1|   A|    2024-03-06|   Senior Analyst|  1|\n|          2|   B|    2024-06-06|Senior Consultant|  1|\n+-----------+----+--------------+-----------------+---+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_date, col, year, rank\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"EmployeeDataFrame\").getOrCreate()\n",
    "\n",
    "# Define the data\n",
    "data = [\n",
    "    (1, \"A\", \"Jan 6th, 2023\", \"Analyst\"),\n",
    "    (1, \"A\", \"Mar 6th, 2024\", \"Senior Analyst\"),\n",
    "    (2, \"B\", \"Feb 6th, 2023\", \"Consultant\"),\n",
    "    (2, \"B\", \"Jun 6th, 2024\", \"Senior Consultant\"),\n",
    "    (3, \"C\", \"July 23rd, 2024\", \"Senior Consultant\")\n",
    "]\n",
    "\n",
    "# Define the schema/columns\n",
    "columns = [\"employee_id\", \"name\", \"effective_date\", \"designation\"]\n",
    "\n",
    "# Create the DataFrame\n",
    "df = spark.createDataFrame(data, schema=columns)\n",
    "\n",
    "# Convert 'effective_date' to date format\n",
    "df = df.withColumn(\"effective_date\", to_date(df[\"effective_date\"], \"MMM d'th', yyyy\"))\n",
    "\n",
    "# Filter for the year 2024\n",
    "df_2024 = df.filter(year(col(\"effective_date\")) == 2024)\n",
    "\n",
    "# Define the window specification\n",
    "window_spec = Window.partitionBy(\"employee_id\").orderBy(col(\"effective_date\").desc())\n",
    "\n",
    "# Add the rank column and filter by rank = 1\n",
    "df_ranked = df_2024.withColumn(\"rnk\", rank().over(window_spec)) \\\n",
    "                    .filter(col(\"rnk\") == 1)\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "df_ranked.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b76936af-82df-4893-8fe3-b257897a0e03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Question 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "247ad208-b0b6-459f-a961-8d11ecf0fcf4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "WITH RANK1 AS (\n",
    "    SELECT\n",
    "        employee_id,\n",
    "        name,\n",
    "        effective_date,\n",
    "        designation,\n",
    "        RANK() OVER (PARTITION BY employee_id ORDER BY effective_date DESC) AS rnk\n",
    "    FROM df\n",
    "    WHERE YEAR(CAST(effective_date AS DATE)) = 2024\n",
    ")\n",
    "SELECT\n",
    "    employee_id,\n",
    "    name,\n",
    "    effective_date,\n",
    "    designation\n",
    "FROM RANK1\n",
    "WHERE rnk = 1;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c629c858-c86f-4daf-913d-772108aca1f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Question 18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a8bb3361-41da-4525-b20e-64bf5f65a9fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Find the moving average for a week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e21bfc6-7314-4e42-9781-c219a75e93d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+----------+----------+------+\n| id|          name|     phone|visited_on|amount|\n+---+--------------+----------+----------+------+\n|  1|         Julia|1234567890|2015-05-01|   100|\n|  2|      Samantha|1234567890|2015-05-02|   200|\n|  3|Julia-Samantha|1234567890|2015-05-03|   300|\n+---+--------------+----------+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import IntegerType, StringType, DateType, StructField, StructType\n",
    "\n",
    "# Define the schema\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"phone\", StringType(), True),\n",
    "    StructField(\"visited_on\", StringType(), True),\n",
    "    StructField(\"amount\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Create the data\n",
    "data = [\n",
    "    (1, \"Julia\", \"1234567890\", \"2015-05-01\", 100),\n",
    "    (2, \"Samantha\", \"1234567890\", \"2015-05-02\", 200),\n",
    "    (3, \"Julia-Samantha\", \"1234567890\", \"2015-05-03\", 300)\n",
    "]\n",
    "\n",
    "# Create a DataFrame\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "181a3366-f551-4924-849a-0ae4e436cefe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+----------+----------+------+--------------+\n| id|          name|     phone|visited_on|amount|moving_average|\n+---+--------------+----------+----------+------+--------------+\n|  1|         Julia|1234567890|2015-05-01|   100|         100.0|\n|  3|Julia-Samantha|1234567890|2015-05-03|   300|         300.0|\n|  2|      Samantha|1234567890|2015-05-02|   200|         200.0|\n+---+--------------+----------+----------+------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Import necessary functions\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import avg\n",
    "df.createOrReplaceTempView(\"df_sql_table\")\n",
    "# Define the window specification\n",
    "window_spec = Window.partitionBy(\"name\").orderBy(\"visited_on\").rowsBetween(-6, 0)\n",
    "# window_spec = Window.partitionBy(\"name\").orderBy(\"visited_on\").rowsBetween(window.unboundedPreceeding, window.currentRow)\n",
    "\n",
    "# Calculate the moving average and create a new column\n",
    "solution_df = df.withColumn(\"moving_average\", avg(\"amount\").over(window_spec))\n",
    "\n",
    "# Show the result\n",
    "solution_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82bc8950-0d2b-40ee-a73a-df1b55722c7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>name</th><th>phone</th><th>visited_on</th><th>amount</th><th>moving_average</th></tr></thead><tbody><tr><td>1</td><td>Julia</td><td>1234567890</td><td>2015-05-01</td><td>100</td><td>100.0</td></tr><tr><td>3</td><td>Julia-Samantha</td><td>1234567890</td><td>2015-05-03</td><td>300</td><td>300.0</td></tr><tr><td>2</td><td>Samantha</td><td>1234567890</td><td>2015-05-02</td><td>200</td><td>200.0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "Julia",
         "1234567890",
         "2015-05-01",
         100,
         100.0
        ],
        [
         3,
         "Julia-Samantha",
         "1234567890",
         "2015-05-03",
         300,
         300.0
        ],
        [
         2,
         "Samantha",
         "1234567890",
         "2015-05-02",
         200,
         200.0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "phone",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "visited_on",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "amount",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "moving_average",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT *, AVG(amount) OVER (PARTITION BY name ORDER BY visited_on ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) AS moving_average\n",
    "FROM df_sql_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ed67543-8f45-4c14-b6f5-d9e7cdb52600",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Question 19 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc84ef6b-41c8-467d-bf4c-7644a2af4c00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Given the product and invoice details for products at an online store, find all the products that were not sold. For each such product, display its SKU and product name. Order the result by SKU, ascending "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3b7782d-3ff2-4b34-917c-f45c17810dfa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+------------+--------------------+-------------+-----------------+---------+\n|product_id|   sku|product_name| product_description|current_price|quantity_in_stock|is_active|\n+----------+------+------------+--------------------+-------------+-----------------+---------+\n|         1|SKU001|      Laptop|High performance ...|      1000.00|            50.00|        1|\n|         2|SKU002|  Smartphone|Latest smartphone...|       800.00|           150.00|        1|\n|         3|SKU003|      Tablet|   Affordable tablet|       300.00|           200.00|        1|\n+----------+------+------------+--------------------+-------------+-----------------+---------+\n\n+----------+----------+--------+-------+----------------+\n|invoice_id|product_id|quantity|  price|line_total_price|\n+----------+----------+--------+-------+----------------+\n|      1001|         1|    2.00|1000.00|         2000.00|\n|      1001|         2|    1.00| 800.00|          800.00|\n|      1002|         3|    3.00| 300.00|          900.00|\n+----------+----------+--------+-------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DecimalType\n",
    "from decimal import Decimal\n",
    "\n",
    "# Define PRODUCT table schema\n",
    "product_schema = StructType([\n",
    "    StructField(\"product_id\", IntegerType(), True),\n",
    "    StructField(\"sku\", StringType(), True),\n",
    "    StructField(\"product_name\", StringType(), True),\n",
    "    StructField(\"product_description\", StringType(), True),\n",
    "    StructField(\"current_price\", DecimalType(8, 2), True),\n",
    "    StructField(\"quantity_in_stock\", DecimalType(8, 2), True),\n",
    "    StructField(\"is_active\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Sample data for PRODUCT table\n",
    "product_data = [\n",
    "    (1, \"SKU001\", \"Laptop\", \"High performance laptop\", Decimal('1000.00'), Decimal('50.00'), 1),\n",
    "    (2, \"SKU002\", \"Smartphone\", \"Latest smartphone model\", Decimal('800.00'), Decimal('150.00'), 1),\n",
    "    (3, \"SKU003\", \"Tablet\", \"Affordable tablet\", Decimal('300.00'), Decimal('200.00'), 1)\n",
    "]\n",
    "\n",
    "# Create PRODUCT DataFrame\n",
    "product_df = spark.createDataFrame(product_data, schema=product_schema)\n",
    "\n",
    "# Show PRODUCT DataFrame\n",
    "product_df.show()\n",
    "\n",
    "# Define INVOICE_ITEM table schema\n",
    "invoice_item_schema = StructType([\n",
    "    StructField(\"invoice_id\", IntegerType(), True),\n",
    "    StructField(\"product_id\", IntegerType(), True),\n",
    "    StructField(\"quantity\", DecimalType(8, 2), True),\n",
    "    StructField(\"price\", DecimalType(8, 2), True),\n",
    "    StructField(\"line_total_price\", DecimalType(8, 2), True)\n",
    "])\n",
    "\n",
    "# Sample data for INVOICE_ITEM table\n",
    "invoice_item_data = [\n",
    "    (1001, 1, Decimal('2.00'), Decimal('1000.00'), Decimal('2000.00')),\n",
    "    (1001, 2, Decimal('1.00'), Decimal('800.00'), Decimal('800.00')),\n",
    "    (1002, 3, Decimal('3.00'), Decimal('300.00'), Decimal('900.00'))\n",
    "]\n",
    "\n",
    "# Create INVOICE_ITEM DataFrame\n",
    "invoice_item_df = spark.createDataFrame(invoice_item_data, schema=invoice_item_schema)\n",
    "\n",
    "# Show INVOICE_ITEM DataFrame\n",
    "invoice_item_df.show()\n",
    "\n",
    "invoice_item_df.createOrReplaceTempView(\"invoice\")\n",
    "product_df.createOrReplaceTempView(\"products\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "850c05f6-142f-4e56-9f46-59e97fe40d7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+\n|sku|product_name|\n+---+------------+\n+---+------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Aliasing the DataFrames\n",
    "df1 = product_df.alias(\"df1\")\n",
    "df2 = invoice_item_df.alias(\"df2\")\n",
    "\n",
    "# Performing the left join\n",
    "join_data_df = df1.join(df2, df1[\"product_id\"] == df2[\"product_id\"], \"left\") \\\n",
    "    .where(df1[\"product_id\"] == 'NULL') \\\n",
    "    .select(col(\"df1.sku\"), col(\"df1.product_name\"))\n",
    "\n",
    "# Showing the result\n",
    "join_data_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "314cecd9-c37d-4431-963a-0c004313634d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>sku</th><th>product_name</th></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "sku",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "product_name",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT p.sku, p.product_name\n",
    "FROM products p\n",
    "LEFT JOIN invoice i ON p.product_id = i.product_id\n",
    "WHERE i.product_id IS NULL\n",
    "ORDER BY p.sku ASC;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d53d1263-646b-497a-a031-87ac1e21fecb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Question 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "884bacd3-a12d-4b23-884f-7d725bd50ce0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Find the highest score obtained by each candidate out of different event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "060b9ec8-5373-46fc-b233-43bf3652675c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+-----+\n|event_id|participant_name|score|\n+--------+----------------+-----+\n|    2187|       Clemencia|  9.0|\n|    2187|       Clemencia|  6.6|\n|    2187|       Clemencia|  8.6|\n|    2187|        Susannah|  8.8|\n+--------+----------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import IntegerType, StringType, DecimalType, StructField, StructType,FloatType\n",
    "\n",
    "\n",
    "# Define the schema for the scoretable\n",
    "schema = StructType([\n",
    "    StructField(\"event_id\", IntegerType(), True),\n",
    "    StructField(\"participant_name\", StringType(), True),\n",
    "    StructField(\"score\", FloatType(), True)  # 3 digits, 1 after decimal point\n",
    "])\n",
    "\n",
    "# Sample data for scoretable\n",
    "data = [\n",
    "    (2187, \"Clemencia\", 9.0),\n",
    "    (2187, \"Clemencia\", 6.6),\n",
    "    (2187, \"Clemencia\", 8.6),\n",
    "    (2187, \"Susannah\", 8.8)\n",
    "]\n",
    "\n",
    "# Create the DataFrame\n",
    "scoretable_df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "# Show the DataFrame\n",
    "scoretable_df.show()\n",
    "\n",
    "scoretable_df.createOrReplaceTempView(\"scoretable_Table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff115057-4aa1-45f1-91f4-90b087efcfec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+---------+\n|event_id|participant_name|max_score|\n+--------+----------------+---------+\n|    2187|       Clemencia|      9.0|\n|    2187|        Susannah|      8.8|\n+--------+----------------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "scoretable_df.groupBy(\"event_id\", \"participant_name\").agg(max(\"score\").alias(\"max_score\")).orderBy(\"max_score\", ascending = False).limit(2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02065681-e66a-4f02-a3b5-af708ac14b29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>event_id</th><th>participant_name</th><th>max_score</th></tr></thead><tbody><tr><td>2187</td><td>Clemencia</td><td>9.0</td></tr><tr><td>2187</td><td>Susannah</td><td>8.8</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         2187,
         "Clemencia",
         9.0
        ],
        [
         2187,
         "Susannah",
         8.8
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "event_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "participant_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "max_score",
         "type": "\"float\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT event_id, participant_name, MAX(score) AS max_score\n",
    "FROM scoretable_Table\n",
    "GROUP BY event_id, participant_name\n",
    "ORDER BY max_score DESC\n",
    "LIMIT 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4502bdb0-a3a6-432c-a31c-3386a39d3553",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Question 21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85d7c9fd-06f9-484a-9a49-2b80b582ff4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. find the the candidate who secured 1,2,3rd rank in different event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4061380-3d04-4735-866a-9a425e526448",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+------------+------------+\n|event_id|rank_1_names|rank_2_names|rank_3_names|\n+--------+------------+------------+------------+\n|2187    |Clemencia   |Susannah    |            |\n+--------+------------+------------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import col, max as spark_max, dense_rank, collect_list, sort_array, concat_ws, when\n",
    "\n",
    "# Step 1: Calculate the highest score per participant for each event\n",
    "ranked_scores_df = scoretable_df.groupBy(\"event_id\", \"participant_name\") \\\n",
    "    .agg(spark_max(\"score\").alias(\"highest_score\"))\n",
    "\n",
    "# Step 2: Apply dense rank based on the highest score for each event\n",
    "window_spec = Window.partitionBy(\"event_id\").orderBy(col(\"highest_score\").desc())\n",
    "\n",
    "ranked_participants_df = ranked_scores_df.withColumn(\n",
    "    \"participant_rank\", dense_rank().over(window_spec)\n",
    ")\n",
    "\n",
    "# Step 3: Filter to only include ranks 1, 2, and 3\n",
    "top_ranks_df = ranked_participants_df.filter(col(\"participant_rank\") <= 3)\n",
    "\n",
    "# Step 4: Group by event_id and collect participant names for each rank, sorted alphabetically\n",
    "ranked_grouped_df = top_ranks_df.groupBy(\"event_id\").agg(\n",
    "    # Collect and concatenate names for rank 1\n",
    "    concat_ws(', ', sort_array(collect_list(when(col(\"participant_rank\") == 1, col(\"participant_name\"))))).alias(\"rank_1_names\"),\n",
    "    # Collect and concatenate names for rank 2\n",
    "    concat_ws(', ', sort_array(collect_list(when(col(\"participant_rank\") == 2, col(\"participant_name\"))))).alias(\"rank_2_names\"),\n",
    "    # Collect and concatenate names for rank 3\n",
    "    concat_ws(', ', sort_array(collect_list(when(col(\"participant_rank\") == 3, col(\"participant_name\"))))).alias(\"rank_3_names\")\n",
    ")\n",
    "\n",
    "# Step 5: Order the results by event_id\n",
    "ranked_grouped_df = ranked_grouped_df.orderBy(\"event_id\")\n",
    "\n",
    "# Show the result\n",
    "ranked_grouped_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d56945f3-6d9c-44e6-9c1f-af95d203082c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>event_id</th><th>rank_1_names</th><th>rank_2_names</th><th>rank_3_names</th></tr></thead><tbody><tr><td>2187</td><td>Clemencia</td><td>Susannah</td><td></td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         2187,
         "Clemencia",
         "Susannah",
         ""
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "event_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "rank_1_names",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "rank_2_names",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "rank_3_names",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "WITH ranked_scores AS (\n",
    "    -- Get the highest score for each participant in each event\n",
    "    SELECT \n",
    "        event_id, \n",
    "        participant_name,\n",
    "        MAX(score) AS highest_score\n",
    "    FROM scoretable_Table\n",
    "    GROUP BY event_id, participant_name\n",
    "), ranked_participants AS (\n",
    "    -- Rank participants based on their highest score in each event\n",
    "    SELECT \n",
    "        event_id,\n",
    "        participant_name,\n",
    "        highest_score,\n",
    "        DENSE_RANK() OVER (PARTITION BY event_id ORDER BY highest_score DESC) AS participant_rank\n",
    "    FROM ranked_scores\n",
    "),\n",
    "top_ranks AS (\n",
    "    -- Select only top 3 ranked participants\n",
    "    SELECT \n",
    "        event_id,\n",
    "        participant_name,\n",
    "        participant_rank\n",
    "    FROM ranked_participants\n",
    "    WHERE participant_rank <= 3\n",
    ")\n",
    "-- Group participants by rank and event\n",
    "SELECT \n",
    "    event_id,\n",
    "    -- Rank 1 names\n",
    "    CONCAT_WS(', ', SORT_ARRAY(COLLECT_LIST(CASE WHEN participant_rank = 1 THEN participant_name END))) AS rank_1_names,\n",
    "    -- Rank 2 names\n",
    "    CONCAT_WS(', ', SORT_ARRAY(COLLECT_LIST(CASE WHEN participant_rank = 2 THEN participant_name END))) AS rank_2_names,\n",
    "    -- Rank 3 names\n",
    "    CONCAT_WS(', ', SORT_ARRAY(COLLECT_LIST(CASE WHEN participant_rank = 3 THEN participant_name END))) AS rank_3_names\n",
    "FROM top_ranks\n",
    "GROUP BY event_id\n",
    "ORDER BY event_id;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "64df554a-9244-4544-b1bf-389bb1693fe7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Question 22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7099e42a-db0a-408a-8bfa-9962decc1e76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Find the monthly max, min and average temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53552015-d375-4d66-93a1-5e41cc8072c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+----------+\n|record_date|data_type|data_value|\n+-----------+---------+----------+\n| 2020-07-01|      max|        92|\n| 2020-07-01|      min|        71|\n| 2020-07-01|      avg|        74|\n| 2020-07-02|      max|        90|\n| 2020-07-02|      min|        67|\n+-----------+---------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import IntegerType, StringType, DateType, StructField, StructType\n",
    "\n",
    "\n",
    "# Define the schema for the temperature_records table\n",
    "schema = StructType([\n",
    "    StructField(\"record_date\", StringType(), True),\n",
    "    StructField(\"data_type\", StringType(), True),\n",
    "    StructField(\"data_value\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Sample data for temperature_records\n",
    "data = [\n",
    "    (\"2020-07-01\", \"max\", 92),\n",
    "    (\"2020-07-01\", \"min\", 71),\n",
    "    (\"2020-07-01\", \"avg\", 74),\n",
    "    (\"2020-07-02\", \"max\", 90),\n",
    "    (\"2020-07-02\", \"min\", 67)\n",
    "]\n",
    "\n",
    "# Create a DataFrame\n",
    "temperature_records_df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "# Show the DataFrame\n",
    "temperature_records_df.show()\n",
    "\n",
    "temperature_records_df.createOrReplaceTempView(\"temperature_records_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "813952cf-c908-4c1b-b62b-2578e5155365",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+----------------+----------------+\n|month  |monthly_avg_temp|monthly_min_temp|monthly_max_temp|\n+-------+----------------+----------------+----------------+\n|2020-07|78.8            |67              |92              |\n+-------+----------------+----------------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import date_format, col, avg, min as spark_min, max as spark_max, when\n",
    "\n",
    "# Step 1: Add a 'month' column and format the 'record_date' to 'yyyy-MM'\n",
    "monthly_temps_df = temperature_records_df.withColumn(\n",
    "    \"month\", date_format(col(\"record_date\"), 'yyyy-MM')\n",
    ")\n",
    "\n",
    "# Step 2: Group by 'month' and 'data_type', and calculate the required aggregations\n",
    "monthly_aggregates_df = monthly_temps_df.groupBy(\"month\").agg(\n",
    "    # Calculate average temperature\n",
    "    avg(\"data_value\").alias(\"monthly_avg_temp\"),\n",
    "    \n",
    "    # Calculate minimum value for 'min' data_type\n",
    "    spark_min(when(col(\"data_type\") == \"min\", col(\"data_value\"))).alias(\"monthly_min_temp\"),\n",
    "    \n",
    "    # Calculate maximum value for 'max' data_type\n",
    "    spark_max(when(col(\"data_type\") == \"max\", col(\"data_value\"))).alias(\"monthly_max_temp\")\n",
    ")\n",
    "\n",
    "# Show the result\n",
    "monthly_aggregates_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "220dea06-e22b-42fd-b7d9-294727674f96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Question 23"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c82f5fae-9df6-455c-9a4f-9dc9f68f29f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "How can you efficiently perform the above steps using PySpark and spark sql to generate a report that meets the requirements specified, while ensuring correct handling of renaming columns, joining multiple DataFrames, calculating aggregations, and filtering based on the global average invoice amount?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc247608-be02-4dfd-8135-f17ca441236f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+\n| id| country_name|\n+---+-------------+\n|  1|United States|\n|  2|       Canada|\n|  3|       Mexico|\n+---+-------------+\n\n+---+-----------+-----------+----------+\n| id|  city_name|postal_code|country_id|\n+---+-----------+-----------+----------+\n|  1|   New York|      10001|         1|\n|  2|Los Angeles|      90001|         1|\n|  3|    Toronto|        M5H|         2|\n|  4|  Vancouver|        V6B|         2|\n|  5|Mexico City|      01000|         3|\n+---+-----------+-----------+----------+\n\n+---+--------------+-------+-----------------+--------------+--------------------+------------+---------+\n| id| customer_name|city_id| customer_address|contact_person|               email|       phone|is_active|\n+---+--------------+-------+-----------------+--------------+--------------------+------------+---------+\n|  1|      John Doe|      1|      123 Main St|      Jane Doe| johndoe@example.com|123-456-7890|        1|\n|  2|     Acme Corp|      2|456 Industrial Rd|     Bob Smith|bob.smith@acmecor...|987-654-3210|        1|\n|  3|Tech Solutions|      3|     789 Tech Ave| Alice Johnson|alice.johnson@tec...|555-123-4567|        1|\n+---+--------------+-------+-----------------+--------------+--------------------+------------+---------+\n\n+---+--------------+-----------+---------------+-----------+\n| id|invoice_number|customer_id|user_account_id|total_price|\n+---+--------------+-----------+---------------+-----------+\n|  1|      INV-1001|          1|            101|      500.0|\n|  2|      INV-1002|          2|            102|     1500.5|\n|  3|      INV-1003|          3|            103|     250.75|\n|  4|      INV-1004|          1|            101|     1000.0|\n+---+--------------+-----------+---------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import IntegerType, StringType, StructField, StructType\n",
    "\n",
    "# Define schema for country table\n",
    "country_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"country_name\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Sample data for country table\n",
    "country_data = [\n",
    "    (1, \"United States\"),\n",
    "    (2, \"Canada\"),\n",
    "    (3, \"Mexico\")\n",
    "]\n",
    "\n",
    "# Create the country DataFrame\n",
    "country_df = spark.createDataFrame(country_data, schema=country_schema)\n",
    "\n",
    "# Show country DataFrame\n",
    "country_df.show()\n",
    "\n",
    "country_df.createOrReplaceTempView(\"country\")\n",
    "# Define schema for city table\n",
    "city_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"city_name\", StringType(), True),\n",
    "    StructField(\"postal_code\", StringType(), True),\n",
    "    StructField(\"country_id\", IntegerType(), True)  # Foreign key referencing country.id\n",
    "])\n",
    "\n",
    "# Sample data for city table\n",
    "city_data = [\n",
    "    (1, \"New York\", \"10001\", 1),\n",
    "    (2, \"Los Angeles\", \"90001\", 1),\n",
    "    (3, \"Toronto\", \"M5H\", 2),\n",
    "    (4, \"Vancouver\", \"V6B\", 2),\n",
    "    (5, \"Mexico City\", \"01000\", 3)\n",
    "]\n",
    "\n",
    "# Create the city DataFrame\n",
    "city_df = spark.createDataFrame(city_data, schema=city_schema)\n",
    "\n",
    "# Show city DataFrame\n",
    "city_df.show()\n",
    "city_df.createOrReplaceTempView(\"city\")\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import IntegerType, StringType, DecimalType, StructField, StructType, FloatType\n",
    "\n",
    "\n",
    "# Define schema for customer table\n",
    "customer_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"customer_name\", StringType(), True),\n",
    "    StructField(\"city_id\", IntegerType(), True),  # Foreign key referencing city.id\n",
    "    StructField(\"customer_address\", StringType(), True),\n",
    "    StructField(\"contact_person\", StringType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"phone\", StringType(), True),\n",
    "    StructField(\"is_active\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Sample data for customer table\n",
    "customer_data = [\n",
    "    (1, \"John Doe\", 1, \"123 Main St\", \"Jane Doe\", \"johndoe@example.com\", \"123-456-7890\", 1),\n",
    "    (2, \"Acme Corp\", 2, \"456 Industrial Rd\", \"Bob Smith\", \"bob.smith@acmecorp.com\", \"987-654-3210\", 1),\n",
    "    (3, \"Tech Solutions\", 3, \"789 Tech Ave\", \"Alice Johnson\", \"alice.johnson@techsol.com\", \"555-123-4567\", 1)\n",
    "]\n",
    "\n",
    "# Create the customer DataFrame\n",
    "customer_df = spark.createDataFrame(customer_data, schema=customer_schema)\n",
    "\n",
    "# Show customer DataFrame\n",
    "customer_df.show()\n",
    "customer_df.createOrReplaceTempView(\"customer\")\n",
    "# Define schema for invoice table\n",
    "invoice_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"invoice_number\", StringType(), True),\n",
    "    StructField(\"customer_id\", IntegerType(), True),  # Foreign key referencing customer.id\n",
    "    StructField(\"user_account_id\", IntegerType(), True),\n",
    "    StructField(\"total_price\", FloatType(), True)\n",
    "])\n",
    "\n",
    "# Sample data for invoice table\n",
    "invoice_data = [\n",
    "    (1, \"INV-1001\", 1, 101, 500.00),\n",
    "    (2, \"INV-1002\", 2, 102, 1500.50),\n",
    "    (3, \"INV-1003\", 3, 103, 250.75),\n",
    "    (4, \"INV-1004\", 1, 101, 1000.00)\n",
    "]\n",
    "\n",
    "# Create the invoice DataFrame\n",
    "invoice_df = spark.createDataFrame(invoice_data, schema=invoice_schema)\n",
    "\n",
    "# Show invoice DataFrame\n",
    "invoice_df.show()\n",
    "invoice_df.createOrReplaceTempView(\"invoice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fabaf17-e537-4daf-9911-ad4555ac2644",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>country_name</th><th>total_invoices</th><th>avg_invoice_amount</th></tr></thead><tbody><tr><td>United States</td><td>3</td><td>1000.166667</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "United States",
         3,
         1000.166667
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "country_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "total_invoices",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "avg_invoice_amount",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "WITH CountryInvoiceData AS (\n",
    "    SELECT\n",
    "        co.country_name,\n",
    "        COUNT(i.id) AS total_invoices,  -- Corrected to i.id\n",
    "        AVG(i.total_price) AS avg_invoice_amount  -- Corrected to i.total_price\n",
    "    FROM\n",
    "        country co\n",
    "    JOIN\n",
    "        city ci ON co.id = ci.country_id\n",
    "    JOIN\n",
    "        customer cu ON ci.id = cu.city_id\n",
    "    JOIN\n",
    "        invoice i ON cu.id = i.customer_id\n",
    "    GROUP BY\n",
    "        co.country_name\n",
    "),\n",
    "GlobalAvg AS (\n",
    "    SELECT\n",
    "        AVG(total_price) AS global_avg_invoice_amount  -- Corrected to total_price\n",
    "    FROM\n",
    "        invoice\n",
    ")\n",
    "SELECT\n",
    "    cid.country_name,\n",
    "    cid.total_invoices,\n",
    "    ROUND(cid.avg_invoice_amount, 6) AS avg_invoice_amount\n",
    "FROM\n",
    "    CountryInvoiceData cid,\n",
    "    GlobalAvg ga\n",
    "WHERE\n",
    "    cid.avg_invoice_amount > ga.global_avg_invoice_amount;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ec4e236-9fa7-42ed-ad0d-349908145485",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------+------------------+\n| country_name|total_invoices|avg_invoice_amount|\n+-------------+--------------+------------------+\n|United States|             3|       1000.166667|\n+-------------+--------------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Step 1: Rename the `id` columns in each DataFrame to make them unique\n",
    "country_df = country_df.withColumnRenamed(\"id\", \"country_id\")\n",
    "city_df = city_df.withColumnRenamed(\"id\", \"city_id\").withColumnRenamed(\"country_id\", \"city_country_id\")\n",
    "customer_df = customer_df.withColumnRenamed(\"id\", \"customer_id\").withColumnRenamed(\"city_id\", \"customer_city_id\")\n",
    "invoice_df = invoice_df.withColumnRenamed(\"id\", \"invoice_id\").withColumnRenamed(\"customer_id\", \"invoice_customer_id\")\n",
    "\n",
    "# Step 2: Join the DataFrames\n",
    "# Join country and city on country_id\n",
    "country_city_df = country_df.join(city_df, country_df.country_id == city_df.city_country_id, \"inner\") \\\n",
    "                            .drop(\"city_country_id\")\n",
    "\n",
    "# Join the result with customer on city_id\n",
    "city_customer_df = country_city_df.join(customer_df, city_df.city_id == customer_df.customer_city_id, \"inner\") \\\n",
    "                                  .drop(\"customer_city_id\")\n",
    "\n",
    "# Join the result with invoice on customer_id\n",
    "customer_invoice_df = city_customer_df.join(invoice_df, customer_df.customer_id == invoice_df.invoice_customer_id, \"inner\") \\\n",
    "                                      .drop(\"invoice_customer_id\")\n",
    "\n",
    "# Step 3: Calculate total invoices and average invoice amount for each country\n",
    "country_invoice_data_df = customer_invoice_df.groupBy(\"country_name\") \\\n",
    "    .agg(\n",
    "        F.count(\"invoice_id\").alias(\"total_invoices\"),  # Count distinct invoices\n",
    "        F.avg(\"total_price\").alias(\"avg_invoice_amount\")  # Calculate average invoice amount\n",
    "    )\n",
    "\n",
    "# Step 4: Calculate the global average invoice amount\n",
    "global_avg_df = invoice_df.agg(F.avg(\"total_price\").alias(\"global_avg_invoice_amount\"))\n",
    "\n",
    "# Step 5: Filter countries where avg_invoice_amount > global_avg_invoice_amount\n",
    "filtered_df = country_invoice_data_df.crossJoin(global_avg_df) \\\n",
    "    .filter(country_invoice_data_df.avg_invoice_amount > global_avg_df.global_avg_invoice_amount)\n",
    "\n",
    "# Step 6: Round the average invoice amount to 6 decimal places\n",
    "final_df = filtered_df.withColumn(\"avg_invoice_amount\", F.round(\"avg_invoice_amount\", 6))\n",
    "\n",
    "# Step 7: Display the result\n",
    "final_df.select(\"country_name\", \"total_invoices\", \"avg_invoice_amount\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc390260-10ca-4c4f-a881-09a2161a5aba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Question 24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a551ff8-b2f8-4dcc-95bd-c42fef291292",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Find the candidate who is getting more salary than 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18075cc7-8e2a-4fb0-97a7-39f68ac31b3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+-----------+\n|employee_ID|            name|   division|\n+-----------+----------------+-----------+\n|        101|   Alice Johnson|      Sales|\n|        102|       Bob Smith|  Marketing|\n|        103|Charlie Williams|Engineering|\n|        104|     David Brown|         HR|\n|        105| Mahammed Faizan|         HR|\n+-----------+----------------+-----------+\n\n+-----------+-----+\n|employee_ID|bonus|\n+-----------+-----+\n|        101| 1000|\n|        102| 1500|\n|        103| 1200|\n|        104| 1100|\n|        105| 5100|\n+-----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import IntegerType, StringType, StructField, StructType\n",
    "\n",
    "\n",
    "# Define schema for employee_information table\n",
    "employee_schema = StructType([\n",
    "    StructField(\"employee_ID\", IntegerType(), True),  # Primary key\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"division\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Sample data for employee_information table\n",
    "employee_data = [\n",
    "    (101, \"Alice Johnson\", \"Sales\"),\n",
    "    (102, \"Bob Smith\", \"Marketing\"),\n",
    "    (103, \"Charlie Williams\", \"Engineering\"),\n",
    "    (104, \"David Brown\", \"HR\"),\n",
    "    (105, \"Mahammed Faizan\", \"HR\")\n",
    "]\n",
    "\n",
    "# Create the employee_information DataFrame\n",
    "employee_df = spark.createDataFrame(employee_data, schema=employee_schema)\n",
    "\n",
    "# Show employee_information DataFrame\n",
    "employee_df.show()\n",
    "\n",
    "employee_df.createOrReplaceTempView(\"employee\")\n",
    "# Define schema for last_quarter_bonus table\n",
    "bonus_schema = StructType([\n",
    "    StructField(\"employee_ID\", IntegerType(), True),  # Primary key, also a foreign key referencing employee_information.employee_ID\n",
    "    StructField(\"bonus\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Sample data for last_quarter_bonus table\n",
    "bonus_data = [\n",
    "    (101, 1000),\n",
    "    (102, 1500),\n",
    "    (103, 1200),\n",
    "    (104, 1100),\n",
    "    (105, 5100)\n",
    "]\n",
    "\n",
    "# Create the last_quarter_bonus DataFrame\n",
    "bonus_df = spark.createDataFrame(bonus_data, schema=bonus_schema)\n",
    "\n",
    "# Show last_quarter_bonus DataFrame\n",
    "bonus_df.show()\n",
    "bonus_df.createOrReplaceTempView(\"bonus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c516964b-c97d-460e-a249-f1ccd31bec09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+\n|employee_ID|           name|\n+-----------+---------------+\n|        105|Mahammed Faizan|\n+-----------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "result_df = employee_df.join(bonus_df, 'employee_ID', 'inner') \\\n",
    "    .select(\"employee_ID\", \"name\") \\\n",
    "    .where((col(\"bonus\") >= 5000) & (col(\"division\") == \"HR\"))\n",
    "\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43879ecb-6f3f-4dd2-8d15-94ff32ff7356",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>employee_ID</th><th>name</th></tr></thead><tbody><tr><td>105</td><td>Mahammed Faizan</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         105,
         "Mahammed Faizan"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "employee_ID",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT ei.employee_ID, ei.name\n",
    "FROM employee ei\n",
    "JOIN bonus lqb\n",
    "ON ei.employee_ID = lqb.employee_ID\n",
    "WHERE ei.division = 'HR'\n",
    "AND lqb.bonus >= 5000;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f0cf9be-bae6-4d2b-8a99-ada265d6c597",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Question 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc000f21-ab0c-47c7-9f1b-b1af2238489c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    " How would you write a PySpark and spark sql code that creates a new column Full_Name, groups the data by Full_Name and email, and aggregates the counts of different relationship_type values for each person, ordered by Full_Name?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d885d3a6-b8d1-4792-85dc-1d01b79c0679",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+---------------------------+-----------------+\n|first_name|last_name |email                      |relationship_type|\n+----------+----------+---------------------------+-----------------+\n|Terri     |Bartel    |tbartel5@infoseek.co.jp    |family           |\n|Micheline |Bottle    |mbottlee@salon.com         |friends          |\n|Haley     |Bradneck  |hbradneckd@odnoklassniki.ru|acquaintances    |\n|Eugine    |Caroline  |ecaroline4@fastcompany.com |family           |\n|Cal       |Chalder   |cchalder1@ox.ac.uk         |friends          |\n|Riva      |Cregin    |rcreginj@sun.com           |acquaintances    |\n|Othello   |Feeham    |ofeehamg@pinterest.com     |family           |\n|Tabor     |Giacopello|tgiacopelloh@flickr.com    |friends          |\n|Jacklin   |Goodband  |jgoodbandf@vk.com          |acquaintances    |\n|Vikky     |Mersh     |vmersh6@admin.ch           |family           |\n|Sunny     |Nannoni   |snannoni8@adobe.com        |friends          |\n|Brittani  |Oxtiby    |boxtiby7@ftc.gov           |acquaintances    |\n|Mara      |Phelps    |mphelpsa@indiegogo.com     |family           |\n|Lola      |Rizzone   |lrizzoneb@google.de        |friends          |\n|Clarisse  |Rodenhurst|crodenhurst2@woothemes.com |acquaintances    |\n|Hartwell  |Saich     |hsaich0@amazonaws.com      |family           |\n|Alphonse  |Scinelli  |ascinelli9@msu.edu         |friends          |\n|Gilbertina|Tynewell  |gtynewellc@webnode.com     |acquaintances    |\n|Rosie     |Verrills  |rverrills3@patch.com       |family           |\n|Job       |Villa     |jvillai@mtv.com            |friends          |\n+----------+----------+---------------------------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "# Define schema for the input data with the 4 required columns\n",
    "schema = StructType([\n",
    "    StructField(\"first_name\", StringType(), True),\n",
    "    StructField(\"last_name\", StringType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"relationship_type\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Define the input data\n",
    "data = [\n",
    "    (\"Terri\", \"Bartel\", \"tbartel5@infoseek.co.jp\", \"family\"),\n",
    "    (\"Micheline\", \"Bottle\", \"mbottlee@salon.com\", \"friends\"),\n",
    "    (\"Haley\", \"Bradneck\", \"hbradneckd@odnoklassniki.ru\", \"acquaintances\"),\n",
    "    (\"Eugine\", \"Caroline\", \"ecaroline4@fastcompany.com\", \"family\"),\n",
    "    (\"Cal\", \"Chalder\", \"cchalder1@ox.ac.uk\", \"friends\"),\n",
    "    (\"Riva\", \"Cregin\", \"rcreginj@sun.com\", \"acquaintances\"),\n",
    "    (\"Othello\", \"Feeham\", \"ofeehamg@pinterest.com\", \"family\"),\n",
    "    (\"Tabor\", \"Giacopello\", \"tgiacopelloh@flickr.com\", \"friends\"),\n",
    "    (\"Jacklin\", \"Goodband\", \"jgoodbandf@vk.com\", \"acquaintances\"),\n",
    "    (\"Vikky\", \"Mersh\", \"vmersh6@admin.ch\", \"family\"),\n",
    "    (\"Sunny\", \"Nannoni\", \"snannoni8@adobe.com\", \"friends\"),\n",
    "    (\"Brittani\", \"Oxtiby\", \"boxtiby7@ftc.gov\", \"acquaintances\"),\n",
    "    (\"Mara\", \"Phelps\", \"mphelpsa@indiegogo.com\", \"family\"),\n",
    "    (\"Lola\", \"Rizzone\", \"lrizzoneb@google.de\", \"friends\"),\n",
    "    (\"Clarisse\", \"Rodenhurst\", \"crodenhurst2@woothemes.com\", \"acquaintances\"),\n",
    "    (\"Hartwell\", \"Saich\", \"hsaich0@amazonaws.com\", \"family\"),\n",
    "    (\"Alphonse\", \"Scinelli\", \"ascinelli9@msu.edu\", \"friends\"),\n",
    "    (\"Gilbertina\", \"Tynewell\", \"gtynewellc@webnode.com\", \"acquaintances\"),\n",
    "    (\"Rosie\", \"Verrills\", \"rverrills3@patch.com\", \"family\"),\n",
    "    (\"Job\", \"Villa\", \"jvillai@mtv.com\", \"friends\")\n",
    "]\n",
    "\n",
    "# Create the DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.createOrReplaceTempView(\"data_in_sql\")\n",
    "# Show the DataFrame\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d52dbe9-5e21-49c4-8977-4e76f3f1ac96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Full_Name</th><th>email</th><th>family</th><th>acquaintances</th><th>friends</th></tr></thead><tbody><tr><td>Bartel Terri</td><td>tbartel5@infoseek.co.jp</td><td>1</td><td>0</td><td>0</td></tr><tr><td>Bottle Micheline</td><td>mbottlee@salon.com</td><td>0</td><td>0</td><td>1</td></tr><tr><td>Bradneck Haley</td><td>hbradneckd@odnoklassniki.ru</td><td>0</td><td>1</td><td>0</td></tr><tr><td>Caroline Eugine</td><td>ecaroline4@fastcompany.com</td><td>1</td><td>0</td><td>0</td></tr><tr><td>Chalder Cal</td><td>cchalder1@ox.ac.uk</td><td>0</td><td>0</td><td>1</td></tr><tr><td>Cregin Riva</td><td>rcreginj@sun.com</td><td>0</td><td>1</td><td>0</td></tr><tr><td>Feeham Othello</td><td>ofeehamg@pinterest.com</td><td>1</td><td>0</td><td>0</td></tr><tr><td>Giacopello Tabor</td><td>tgiacopelloh@flickr.com</td><td>0</td><td>0</td><td>1</td></tr><tr><td>Goodband Jacklin</td><td>jgoodbandf@vk.com</td><td>0</td><td>1</td><td>0</td></tr><tr><td>Mersh Vikky</td><td>vmersh6@admin.ch</td><td>1</td><td>0</td><td>0</td></tr><tr><td>Nannoni Sunny</td><td>snannoni8@adobe.com</td><td>0</td><td>0</td><td>1</td></tr><tr><td>Oxtiby Brittani</td><td>boxtiby7@ftc.gov</td><td>0</td><td>1</td><td>0</td></tr><tr><td>Phelps Mara</td><td>mphelpsa@indiegogo.com</td><td>1</td><td>0</td><td>0</td></tr><tr><td>Rizzone Lola</td><td>lrizzoneb@google.de</td><td>0</td><td>0</td><td>1</td></tr><tr><td>Rodenhurst Clarisse</td><td>crodenhurst2@woothemes.com</td><td>0</td><td>1</td><td>0</td></tr><tr><td>Saich Hartwell</td><td>hsaich0@amazonaws.com</td><td>1</td><td>0</td><td>0</td></tr><tr><td>Scinelli Alphonse</td><td>ascinelli9@msu.edu</td><td>0</td><td>0</td><td>1</td></tr><tr><td>Tynewell Gilbertina</td><td>gtynewellc@webnode.com</td><td>0</td><td>1</td><td>0</td></tr><tr><td>Verrills Rosie</td><td>rverrills3@patch.com</td><td>1</td><td>0</td><td>0</td></tr><tr><td>Villa Job</td><td>jvillai@mtv.com</td><td>0</td><td>0</td><td>1</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Bartel Terri",
         "tbartel5@infoseek.co.jp",
         1,
         0,
         0
        ],
        [
         "Bottle Micheline",
         "mbottlee@salon.com",
         0,
         0,
         1
        ],
        [
         "Bradneck Haley",
         "hbradneckd@odnoklassniki.ru",
         0,
         1,
         0
        ],
        [
         "Caroline Eugine",
         "ecaroline4@fastcompany.com",
         1,
         0,
         0
        ],
        [
         "Chalder Cal",
         "cchalder1@ox.ac.uk",
         0,
         0,
         1
        ],
        [
         "Cregin Riva",
         "rcreginj@sun.com",
         0,
         1,
         0
        ],
        [
         "Feeham Othello",
         "ofeehamg@pinterest.com",
         1,
         0,
         0
        ],
        [
         "Giacopello Tabor",
         "tgiacopelloh@flickr.com",
         0,
         0,
         1
        ],
        [
         "Goodband Jacklin",
         "jgoodbandf@vk.com",
         0,
         1,
         0
        ],
        [
         "Mersh Vikky",
         "vmersh6@admin.ch",
         1,
         0,
         0
        ],
        [
         "Nannoni Sunny",
         "snannoni8@adobe.com",
         0,
         0,
         1
        ],
        [
         "Oxtiby Brittani",
         "boxtiby7@ftc.gov",
         0,
         1,
         0
        ],
        [
         "Phelps Mara",
         "mphelpsa@indiegogo.com",
         1,
         0,
         0
        ],
        [
         "Rizzone Lola",
         "lrizzoneb@google.de",
         0,
         0,
         1
        ],
        [
         "Rodenhurst Clarisse",
         "crodenhurst2@woothemes.com",
         0,
         1,
         0
        ],
        [
         "Saich Hartwell",
         "hsaich0@amazonaws.com",
         1,
         0,
         0
        ],
        [
         "Scinelli Alphonse",
         "ascinelli9@msu.edu",
         0,
         0,
         1
        ],
        [
         "Tynewell Gilbertina",
         "gtynewellc@webnode.com",
         0,
         1,
         0
        ],
        [
         "Verrills Rosie",
         "rverrills3@patch.com",
         1,
         0,
         0
        ],
        [
         "Villa Job",
         "jvillai@mtv.com",
         0,
         0,
         1
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Full_Name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "email",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "family",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "acquaintances",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "friends",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT\n",
    "    CONCAT(last_name, ' ', first_name) AS Full_Name,\n",
    "    email,\n",
    "    COALESCE(SUM(CASE WHEN relationship_type = 'family' THEN 1 ELSE 0 END), 0) AS family,\n",
    "    COALESCE(SUM(CASE WHEN relationship_type = 'acquaintances' THEN 1 ELSE 0 END), 0) AS acquaintances,\n",
    "    COALESCE(SUM(CASE WHEN relationship_type = 'friends' THEN 1 ELSE 0 END), 0) AS friends\n",
    "FROM \n",
    "    data_in_sql r\n",
    "GROUP BY \n",
    "    last_name, first_name, email\n",
    "ORDER BY \n",
    "    Full_Name;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7779e529-af11-41d3-8b12-bc81f47192a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+------+-------------+-------+\n|          Full_Name|               email|family|acquaintances|friends|\n+-------------------+--------------------+------+-------------+-------+\n|       Bartel Terri|tbartel5@infoseek...|     1|            0|      0|\n|   Bottle Micheline|  mbottlee@salon.com|     0|            0|      1|\n|     Bradneck Haley|hbradneckd@odnokl...|     0|            1|      0|\n|    Caroline Eugine|ecaroline4@fastco...|     1|            0|      0|\n|        Chalder Cal|  cchalder1@ox.ac.uk|     0|            0|      1|\n|        Cregin Riva|    rcreginj@sun.com|     0|            1|      0|\n|     Feeham Othello|ofeehamg@pinteres...|     1|            0|      0|\n|   Giacopello Tabor|tgiacopelloh@flic...|     0|            0|      1|\n|   Goodband Jacklin|   jgoodbandf@vk.com|     0|            1|      0|\n|        Mersh Vikky|    vmersh6@admin.ch|     1|            0|      0|\n|      Nannoni Sunny| snannoni8@adobe.com|     0|            0|      1|\n|    Oxtiby Brittani|    boxtiby7@ftc.gov|     0|            1|      0|\n|        Phelps Mara|mphelpsa@indiegog...|     1|            0|      0|\n|       Rizzone Lola| lrizzoneb@google.de|     0|            0|      1|\n|Rodenhurst Clarisse|crodenhurst2@woot...|     0|            1|      0|\n|     Saich Hartwell|hsaich0@amazonaws...|     1|            0|      0|\n|  Scinelli Alphonse|  ascinelli9@msu.edu|     0|            0|      1|\n|Tynewell Gilbertina|gtynewellc@webnod...|     0|            1|      0|\n|     Verrills Rosie|rverrills3@patch.com|     1|            0|      0|\n|          Villa Job|     jvillai@mtv.com|     0|            0|      1|\n+-------------------+--------------------+------+-------------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, concat, sum, when, lit\n",
    "\n",
    "result_df = df.withColumn(\"Full_Name\", concat(col(\"last_name\"), lit(\" \"), col(\"first_name\"))) \\\n",
    "    .groupBy(\"Full_Name\", \"email\") \\\n",
    "    .agg(\n",
    "        sum(when(col(\"relationship_type\") == 'family', 1).otherwise(0)).alias(\"family\"),\n",
    "        sum(when(col(\"relationship_type\") == 'acquaintances', 1).otherwise(0)).alias(\"acquaintances\"),\n",
    "        sum(when(col(\"relationship_type\") == 'friends', 1).otherwise(0)).alias(\"friends\")\n",
    "    ) \\\n",
    "    .orderBy(\"Full_Name\")\n",
    "\n",
    "# Show the result\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "04df27b0-efcc-452f-a625-d90041d9b5f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Question 26"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d052f45-d547-4112-a09c-0d175acd234b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "A potential scenario question for this code could be:\n",
    "\n",
    "Scenario:\n",
    "You have a JSON file with workout data where some of the records have missing or corrupted fields like id, name, or age. Your task is to clean the data by performing the following:\n",
    "\n",
    "1. If the id field is missing but _corrupt_record exists, extract the id from the corrupted record and update the id field.\n",
    "2. If the name field is missing but _corrupt_record exists, extract the name from the corrupted record and update the name field. If still missing, set it to \"Unknown\".\n",
    "3. If the age field is missing but _corrupt_record exists, extract the numeric value from the corrupted record and update the age field.\n",
    "Drop the _corrupt_record column.\n",
    "4. Ensure that missing name values are replaced with \"Unknown\".\n",
    "You are given a corrupted JSON file (missing_json.json) in Databricks FileStore, which contains the above-mentioned issues. Write a PySpark script to read the data, clean it, and output the cleaned dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e325b512-a15d-43a4-bc0b-b3e908ff3b27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input = \n+-----------------------------+----+----+-------+\n|_corrupt_record              |age |id  |name   |\n+-----------------------------+----+----+-------+\n|null                         |30  |1   |Alice  |\n|{'id\": 2, \"name\": \"Bob\", 40},|null|null|null   |\n|null                         |25  |3   |null   |\n|null                         |40  |4   |Charlie|\n+-----------------------------+----+----+-------+\n\noutput = \n+---+---+-------+\n|age|id |name   |\n+---+---+-------+\n|30 |1  |Alice  |\n|40 |2  |Bob    |\n|25 |3  |Unknown|\n|40 |4  |Charlie|\n+---+---+-------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, regexp_replace\n",
    "\n",
    "df = spark.read.json(\"dbfs:/FileStore/workout_data/missing_json.json\")\n",
    "print(\"Input = \")\n",
    "df.show(truncate=False)\n",
    "\n",
    "cleaned_df = df.withColumn(\"id\", \n",
    "                            when(col(\"id\").isNull() & col(\"_corrupt_record\").isNotNull(), \n",
    "                                 regexp_replace(col(\"_corrupt_record\"), r'.*?\\'id\":?\\s*(\\d+).*', '$1').cast(\"int\")) \\\n",
    "                            .otherwise(col(\"id\"))) \\\n",
    "                .withColumn(\"name\", \n",
    "                            when(col(\"name\").isNull() & col(\"_corrupt_record\").isNotNull(), \n",
    "                                 regexp_replace(col(\"_corrupt_record\"), r'.*?\"name\":\\s*\"([^\"]+)\".*', '$1')) \\\n",
    "                            .otherwise(col(\"name\"))) \\\n",
    "                .withColumn(\"age\", \n",
    "                            when(col(\"age\").isNull() & col(\"_corrupt_record\").isNotNull(), \n",
    "                                 regexp_replace(col(\"_corrupt_record\"), r'.*?(\\d+)[^0-9]*$', '$1').cast(\"int\")) \\\n",
    "                            .otherwise(col(\"age\"))) \\\n",
    "                .drop(\"_corrupt_record\")\n",
    "\n",
    "cleaned_df = cleaned_df.na.fill({\"name\": \"Unknown\"}) \n",
    "print(\"output = \")\n",
    "cleaned_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7e6c141-d558-4a5e-8d56-8ea89742efa1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+\n|_corrupt_record              |\n+-----------------------------+\n|null                         |\n|{'id\": 2, \"name\": \"Bob\", 40},|\n|null                         |\n|null                         |\n+-----------------------------+\n\n+----+----+-------+\n|age |id  |name   |\n+----+----+-------+\n|30  |1   |Alice  |\n|null|null|Bob    |\n|25  |3   |null   |\n|40  |4   |Charlie|\n+----+----+-------+\n\n"
     ]
    }
   ],
   "source": [
    "# Read the JSON data and cache the DataFrame\n",
    "df = spark.read.json(\"dbfs:/FileStore/workout_data/missing_json.json\").cache()\n",
    "\n",
    "# Now, you can inspect the _corrupt_record safely\n",
    "df.select(\"_corrupt_record\").show(truncate=False)\n",
    "\n",
    "# Proceed with fixing the DataFrame\n",
    "df_fixed = df.withColumn(\n",
    "    \"id\", F.coalesce(F.col(\"id\"), F.regexp_extract(F.col(\"_corrupt_record\"), '\"id\":\\\\s*\"([^\"]+)\"', 1).cast(\"int\"))\n",
    ").withColumn(\n",
    "    \"name\", F.coalesce(F.col(\"name\"), F.regexp_extract(F.col(\"_corrupt_record\"), '\"name\":\\\\s*\"([^\"]+)\"', 1))\n",
    ").withColumn(\n",
    "    \"age\", F.coalesce(F.col(\"age\"), F.regexp_extract(F.col(\"_corrupt_record\"), '\"age\":\\\\s*(\\\\d+)', 1).cast(\"int\"))\n",
    ")\n",
    "\n",
    "# Drop _corrupt_record after fixing the data\n",
    "df_fixed = df_fixed.drop(\"_corrupt_record\")\n",
    "\n",
    "# Show the cleaned DataFrame\n",
    "df_fixed.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f07759d6-0175-4388-995b-1cfc4482b6bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Question 27"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "375703ce-2be8-46dd-90d7-37e60af8124a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Flatten the nested JSON structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff7f3399-43ca-4349-a8f5-aa09d9fb106e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>goods</th><th>location</th><th>name</th><th>satellites</th></tr></thead><tbody><tr><td>List(List(government, distributer, retail), List(List(1, 123.34, List(List(List(Laptop, 20), List(Charger, 2)))), List(2, 323.34, List(List(List(Mice, 2), List(Keyboard, 1))))), true)</td><td>Redmond</td><td>MSFT</td><td>List(Bay Area, Shanghai)</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         [
          [
           "government",
           "distributer",
           "retail"
          ],
          [
           [
            1,
            123.34,
            [
             [
              [
               "Laptop",
               20
              ],
              [
               "Charger",
               2
              ]
             ]
            ]
           ],
           [
            2,
            323.34,
            [
             [
              [
               "Mice",
               2
              ],
              [
               "Keyboard",
               1
              ]
             ]
            ]
           ]
          ],
          true
         ],
         "Redmond",
         "MSFT",
         [
          "Bay Area",
          "Shanghai"
         ]
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "goods",
         "type": "{\"type\":\"struct\",\"fields\":[{\"name\":\"customers\",\"type\":{\"type\":\"array\",\"elementType\":\"string\",\"containsNull\":true},\"nullable\":true,\"metadata\":{}},{\"name\":\"orders\",\"type\":{\"type\":\"array\",\"elementType\":{\"type\":\"struct\",\"fields\":[{\"name\":\"orderId\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"orderTotal\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"shipped\",\"type\":{\"type\":\"struct\",\"fields\":[{\"name\":\"orderItems\",\"type\":{\"type\":\"array\",\"elementType\":{\"type\":\"struct\",\"fields\":[{\"name\":\"itemName\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"itemQty\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}}]},\"containsNull\":true},\"nullable\":true,\"metadata\":{}}]},\"nullable\":true,\"metadata\":{}}]},\"containsNull\":true},\"nullable\":true,\"metadata\":{}},{\"name\":\"trade\",\"type\":\"boolean\",\"nullable\":true,\"metadata\":{}}]}"
        },
        {
         "metadata": "{}",
         "name": "location",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "satellites",
         "type": "{\"type\":\"array\",\"elementType\":\"string\",\"containsNull\":true}"
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- goods: struct (nullable = true)\n |    |-- customers: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |    |-- orders: array (nullable = true)\n |    |    |-- element: struct (containsNull = true)\n |    |    |    |-- orderId: long (nullable = true)\n |    |    |    |-- orderTotal: double (nullable = true)\n |    |    |    |-- shipped: struct (nullable = true)\n |    |    |    |    |-- orderItems: array (nullable = true)\n |    |    |    |    |    |-- element: struct (containsNull = true)\n |    |    |    |    |    |    |-- itemName: string (nullable = true)\n |    |    |    |    |    |    |-- itemQty: long (nullable = true)\n |    |-- trade: boolean (nullable = true)\n |-- location: string (nullable = true)\n |-- name: string (nullable = true)\n |-- satellites: array (nullable = true)\n |    |-- element: string (containsNull = true)\n\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"multiLine\", \"true\").json(\"dbfs:/FileStore/workout_data/nested.json\")\n",
    "display(df)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3a03feb-e9a5-4965-be39-eac9b543b3e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- location: string (nullable = true)\n |-- name: string (nullable = true)\n |-- satellites: string (nullable = true)\n |-- customers: string (nullable = true)\n |-- trade: boolean (nullable = true)\n |-- orderId: long (nullable = true)\n |-- orderTotal: double (nullable = true)\n |-- itemName: string (nullable = true)\n |-- itemQty: long (nullable = true)\n\n+--------+----+----------+-----------+-----+-------+----------+--------+-------+\n|location|name|satellites|customers  |trade|orderId|orderTotal|itemName|itemQty|\n+--------+----+----------+-----------+-----+-------+----------+--------+-------+\n|Redmond |MSFT|Bay Area  |government |true |1      |123.34    |Laptop  |20     |\n|Redmond |MSFT|Shanghai  |government |true |1      |123.34    |Laptop  |20     |\n|Redmond |MSFT|Bay Area  |government |true |1      |123.34    |Charger |2      |\n|Redmond |MSFT|Shanghai  |government |true |1      |123.34    |Charger |2      |\n|Redmond |MSFT|Bay Area  |government |true |2      |323.34    |Mice    |2      |\n|Redmond |MSFT|Shanghai  |government |true |2      |323.34    |Mice    |2      |\n|Redmond |MSFT|Bay Area  |government |true |2      |323.34    |Keyboard|1      |\n|Redmond |MSFT|Shanghai  |government |true |2      |323.34    |Keyboard|1      |\n|Redmond |MSFT|Bay Area  |distributer|true |1      |123.34    |Laptop  |20     |\n|Redmond |MSFT|Shanghai  |distributer|true |1      |123.34    |Laptop  |20     |\n|Redmond |MSFT|Bay Area  |distributer|true |1      |123.34    |Charger |2      |\n|Redmond |MSFT|Shanghai  |distributer|true |1      |123.34    |Charger |2      |\n|Redmond |MSFT|Bay Area  |distributer|true |2      |323.34    |Mice    |2      |\n|Redmond |MSFT|Shanghai  |distributer|true |2      |323.34    |Mice    |2      |\n|Redmond |MSFT|Bay Area  |distributer|true |2      |323.34    |Keyboard|1      |\n|Redmond |MSFT|Shanghai  |distributer|true |2      |323.34    |Keyboard|1      |\n|Redmond |MSFT|Bay Area  |retail     |true |1      |123.34    |Laptop  |20     |\n|Redmond |MSFT|Shanghai  |retail     |true |1      |123.34    |Laptop  |20     |\n|Redmond |MSFT|Bay Area  |retail     |true |1      |123.34    |Charger |2      |\n|Redmond |MSFT|Shanghai  |retail     |true |1      |123.34    |Charger |2      |\n+--------+----+----------+-----------+-----+-------+----------+--------+-------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Sample data loading (replace this with your actual DataFrame)\n",
    "# df = spark.read.json(\"path_to_your_data.json\")\n",
    "\n",
    "# Flattening the 'goods' struct\n",
    "df_flattened = df \\\n",
    "    .withColumn(\"customers\", F.explode_outer(\"goods.customers\")) \\\n",
    "    .withColumn(\"orders\", F.explode_outer(\"goods.orders\")) \\\n",
    "    .withColumn(\"trade\", F.col(\"goods.trade\")) \\\n",
    "    .withColumn(\"orderId\", F.col(\"orders.orderId\")) \\\n",
    "    .withColumn(\"orderTotal\", F.col(\"orders.orderTotal\")) \\\n",
    "    .withColumn(\"shipped_items\", F.explode_outer(\"orders.shipped.orderItems\")) \\\n",
    "    .withColumn(\"itemName\", F.col(\"shipped_items.itemName\")) \\\n",
    "    .withColumn(\"itemQty\", F.col(\"shipped_items.itemQty\")) \\\n",
    "    .withColumn(\"location\", F.col(\"location\")) \\\n",
    "    .withColumn(\"name\", F.col(\"name\")) \\\n",
    "    .withColumn(\"satellites\", F.explode_outer(\"satellites\"))\n",
    "\n",
    "# Dropping the original nested columns to keep only flattened ones\n",
    "df_flattened = df_flattened.drop(\"goods\", \"orders\", \"shipped_items\")\n",
    "\n",
    "# Show the flattened schema and data\n",
    "df_flattened.printSchema()\n",
    "df_flattened.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4806cb0-589e-4888-9c42-4325e2343098",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Question 28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e35e5ee-c483-40a0-97a2-7a6c2bd26059",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Problem:\n",
    "1. Write a PySpark job to identify \"underperforming\" products in each warehouse. A product is considered underperforming in a warehouse if:\n",
    "\n",
    "1a. It has a stock level above the ReorderPoint.\n",
    "1b. It has had no sales in the last 30 days. (last_sale_date - current_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7facd390-cb1c-420e-a5ae-b3b49b4f3d27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying sales_df\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>SaleID</th><th>ProductID</th><th>WarehouseID</th><th>SaleDate</th><th>QuantitySold</th><th>SaleAmount</th></tr></thead><tbody><tr><td>1</td><td>101</td><td>1001</td><td>2024-08-15</td><td>10</td><td>200</td></tr><tr><td>2</td><td>102</td><td>1002</td><td>2024-09-20</td><td>5</td><td>100</td></tr><tr><td>3</td><td>101</td><td>1001</td><td>2024-07-10</td><td>15</td><td>300</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         101,
         1001,
         "2024-08-15",
         10,
         200
        ],
        [
         2,
         102,
         1002,
         "2024-09-20",
         5,
         100
        ],
        [
         3,
         101,
         1001,
         "2024-07-10",
         15,
         300
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "SaleID",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "ProductID",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "WarehouseID",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "SaleDate",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "QuantitySold",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "SaleAmount",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying inventory_df\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>ProductID</th><th>WarehouseID</th><th>StockLevel</th><th>ReorderPoint</th></tr></thead><tbody><tr><td>101</td><td>1001</td><td>50</td><td>20</td></tr><tr><td>102</td><td>1002</td><td>30</td><td>25</td></tr><tr><td>103</td><td>1001</td><td>10</td><td>15</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         101,
         1001,
         50,
         20
        ],
        [
         102,
         1002,
         30,
         25
        ],
        [
         103,
         1001,
         10,
         15
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "ProductID",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "WarehouseID",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "StockLevel",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "ReorderPoint",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sales_data = [\n",
    "    (1, 101, 1001, '2024-08-15', 10, 200),\n",
    "    (2, 102, 1002, '2024-09-20', 5, 100),\n",
    "    (3, 101, 1001, '2024-07-10', 15, 300),\n",
    "]\n",
    "\n",
    "inventory_data = [\n",
    "    (101, 1001, 50, 20),\n",
    "    (102, 1002, 30, 25),\n",
    "    (103, 1001, 10, 15),\n",
    "]\n",
    "\n",
    "sales_columns = ['SaleID', 'ProductID', 'WarehouseID', 'SaleDate', 'QuantitySold', 'SaleAmount']\n",
    "inventory_columns = ['ProductID', 'WarehouseID', 'StockLevel', 'ReorderPoint']\n",
    "\n",
    "sales_df = spark.createDataFrame(sales_data, sales_columns)\n",
    "inventory_df = spark.createDataFrame(inventory_data, inventory_columns)\n",
    "\n",
    "print(\"Displaying sales_df\")\n",
    "display(sales_df)\n",
    "print(\"Displaying inventory_df\")\n",
    "display(inventory_df)\n",
    "\n",
    "# Create temporary views for the DataFrames\n",
    "sales_df.createOrReplaceTempView(\"Sales\")\n",
    "inventory_df.createOrReplaceTempView(\"Inventory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "204de795-08aa-4fe4-b091-75f2ff923c48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing final_output\n+---------+-----------+----------+-----------------+\n|ProductID|WarehouseID|StockLevel|DaysSinceLastSale|\n+---------+-----------+----------+-----------------+\n|      101|       1001|        50|               67|\n|      102|       1002|        30|               31|\n+---------+-----------+----------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import col, max, current_date, datediff, date_format\n",
    "\n",
    "# Remove duplicates from sales_df and inventory_df\n",
    "sales_df = sales_df.dropDuplicates([\"ProductID\", \"WarehouseID\", \"SaleDate\"])  \n",
    "inventory_df = inventory_df.dropDuplicates([\"ProductID\", \"WarehouseID\"])\n",
    "\n",
    "# Step 1: Filter Inventory for StockLevel above ReorderPoint\n",
    "inventory_filtered = inventory_df.filter(col('StockLevel') > col('ReorderPoint'))\n",
    "\n",
    "# Step 2: Calculate DaysSinceLastSale for each product in the Sales dataset\n",
    "window_spec = Window.partitionBy(\"ProductID\", \"WarehouseID\").orderBy(col(\"SaleDate\").desc())\n",
    "sales_with_last_date = sales_df.withColumn(\"last_sale_date\", max(col(\"SaleDate\")).over(window_spec))\n",
    "sales_with_last_date = sales_with_last_date.withColumn(\"DaysSinceLastSale\", datediff(current_date(), col(\"last_sale_date\")))\n",
    "\n",
    "# Filter products with no sales in the last 30 days\n",
    "sales_filtered = sales_with_last_date.filter(col(\"DaysSinceLastSale\") > 30)\n",
    "\n",
    "# Step 4: Join the filtered inventory data with the sales data to find underperforming products\n",
    "underperforming_products = sales_filtered.join(inventory_filtered, [\"ProductID\", \"WarehouseID\"], \"inner\")\n",
    "\n",
    "# Step 5: Select the necessary columns for the final output and remove duplicates\n",
    "final_output = underperforming_products.select(\"ProductID\", \"WarehouseID\", \"StockLevel\", \"DaysSinceLastSale\").distinct()\n",
    "\n",
    "print(\"printing final_output\")\n",
    "final_output.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65933bcd-8123-4d55-8beb-b4c7c2a371b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>ProductID</th><th>WarehouseID</th><th>StockLevel</th><th>DaysSinceLastSale</th></tr></thead><tbody><tr><td>101</td><td>1001</td><td>50</td><td>67</td></tr><tr><td>102</td><td>1002</td><td>30</td><td>31</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         101,
         1001,
         50,
         67
        ],
        [
         102,
         1002,
         30,
         31
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "ProductID",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "WarehouseID",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "StockLevel",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "DaysSinceLastSale",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "-- SQL Query to find underperforming products\n",
    "WITH InventoryFiltered AS (\n",
    "    -- Step 1: Filter Inventory for StockLevel above ReorderPoint\n",
    "    SELECT ProductID, WarehouseID, StockLevel, ReorderPoint\n",
    "    FROM Inventory\n",
    "    WHERE StockLevel > ReorderPoint\n",
    "),\n",
    "\n",
    "SalesWithLastDate AS (\n",
    "    -- Step 2: Calculate DaysSinceLastSale for each product in the Sales dataset\n",
    "    SELECT \n",
    "        s.ProductID,\n",
    "        s.WarehouseID,\n",
    "        MAX(s.SaleDate) AS last_sale_date\n",
    "    FROM Sales s\n",
    "    GROUP BY s.ProductID, s.WarehouseID\n",
    "),\n",
    "\n",
    "SalesFiltered AS (\n",
    "    -- Step 3: Filter products with no sales in the last 30 days\n",
    "    SELECT \n",
    "        swld.ProductID,\n",
    "        swld.WarehouseID,\n",
    "        swld.last_sale_date,\n",
    "        DATEDIFF(CURRENT_DATE, swld.last_sale_date) AS DaysSinceLastSale\n",
    "    FROM SalesWithLastDate swld\n",
    "    WHERE DATEDIFF(CURRENT_DATE, swld.last_sale_date) > 30\n",
    ")\n",
    "\n",
    "-- Step 4: Join the filtered inventory data with the sales data to find underperforming products\n",
    "SELECT \n",
    "    sf.ProductID,\n",
    "    sf.WarehouseID,\n",
    "    inf.StockLevel,\n",
    "    sf.DaysSinceLastSale\n",
    "FROM SalesFiltered sf\n",
    "JOIN InventoryFiltered inf\n",
    "ON sf.ProductID = inf.ProductID AND sf.WarehouseID = inf.WarehouseID;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f1b8b5b-4345-4902-91ef-a4080abf66ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Question 29"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "99f3f814-6545-4551-adf6-7069982dacc8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "below queries :\n",
    "Write a SQL query to retrieve the total transaction amounts in USD for each company code for the fiscal year 2023. Convert all transaction amounts to USD using the appropriate exchange rates stored in another table ExchangeRates, which has the columns Currency, ExchangeRate, and\n",
    "EffectiveDate.\n",
    "\n",
    "TransactionAmount in USD= round(sum(TransactionAmount/RE), 2) group by CompanyCode, where FiscalYear = '2023' from ExchangeRates table \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f700ff1a-bd57-4193-aebc-617eaa2c20cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Transactions DataFrame:\n+-----------+----------+-------------+-----------------+--------+------------+\n|CompanyCode|FiscalYear|AccountNumber|TransactionAmount|Currency|DocumentDate|\n+-----------+----------+-------------+-----------------+--------+------------+\n|       US01|      2023|       100001|              500|     USD|  2023-01-15|\n|       DE02|      2023|       200002|              450|     EUR|  2023-03-21|\n|       IN03|      2023|       300003|            35000|     INR|  2023-07-09|\n|       US01|      2022|       100004|             1200|     USD|  2022-05-14|\n|       DE02|      2023|       200005|              700|     EUR|  2023-09-11|\n+-----------+----------+-------------+-----------------+--------+------------+\n\nSample Exchange Rates DataFrame:\n+--------+------------+-------------+\n|Currency|ExchangeRate|EffectiveDate|\n+--------+------------+-------------+\n|     EUR|         1.1|   2023-01-01|\n|     INR|       0.012|   2023-01-01|\n|     USD|         1.0|   2023-01-01|\n+--------+------------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Sample data for transactions_df\n",
    "transactions_data = [\n",
    "    ('US01', 2023, '100001', 500, 'USD', '2023-01-15'),\n",
    "    ('DE02', 2023, '200002', 450, 'EUR', '2023-03-21'),\n",
    "    ('IN03', 2023, '300003', 35000, 'INR', '2023-07-09'),\n",
    "    ('US01', 2022, '100004', 1200, 'USD', '2022-05-14'),\n",
    "    ('DE02', 2023, '200005', 700, 'EUR', '2023-09-11')\n",
    "]\n",
    "\n",
    "# Create the transactions_df DataFrame\n",
    "transactions_df = spark.createDataFrame(transactions_data, \n",
    "    ['CompanyCode', 'FiscalYear', 'AccountNumber', 'TransactionAmount', 'Currency', 'DocumentDate'])\n",
    "\n",
    "# Sample data for exchange_rates_df\n",
    "exchange_rates_data = [\n",
    "    ('EUR', 1.10, '2023-01-01'),\n",
    "    ('INR', 0.012, '2023-01-01'),\n",
    "    ('USD', 1.00, '2023-01-01')\n",
    "]\n",
    "\n",
    "# Create the exchange_rates_df DataFrame\n",
    "exchange_rates_df = spark.createDataFrame(exchange_rates_data, \n",
    "    ['Currency', 'ExchangeRate', 'EffectiveDate'])\n",
    "\n",
    "# Show sample tables\n",
    "print(\"Sample Transactions DataFrame:\")\n",
    "transactions_df.show()\n",
    "\n",
    "print(\"Sample Exchange Rates DataFrame:\")\n",
    "exchange_rates_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88c1d53d-a6fe-4660-9ff8-2c5a16fa9b01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing output result\n+-----------+----------------------------+\n|CompanyCode|total_transaction_amount_usd|\n+-----------+----------------------------+\n|       US01|                       500.0|\n|       DE02|                     1045.45|\n|       IN03|                  2916666.67|\n+-----------+----------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the window specification to get the latest exchange rate per currency\n",
    "window_spec = Window.partitionBy(\"Currency\").orderBy(F.desc(\"EffectiveDate\"))\n",
    "\n",
    "# Step 1: Add a row number to select the most recent exchange rate for each currency\n",
    "latest_exchange_rates_df = exchange_rates_df.withColumn(\n",
    "    \"rn\", F.row_number().over(window_spec)\n",
    ").filter(F.col(\"rn\") == 1)\n",
    "\n",
    "# Step 2: Join the transactions_df with the filtered exchange rates based on currency\n",
    "joined_df = transactions_df.alias(\"a\").join(\n",
    "    latest_exchange_rates_df.alias(\"b\"),\n",
    "    (F.col(\"a.Currency\") == F.col(\"b.Currency\")) &\n",
    "    (F.col(\"b.EffectiveDate\") <= F.col(\"a.DocumentDate\")),  # Filter based on DocumentDate\n",
    "    \"left\"\n",
    ")\n",
    "\n",
    "# Step 3: Calculate total transaction amounts in USD, rounding to 2 decimal places\n",
    "result_df = joined_df.filter(F.col(\"a.FiscalYear\") == 2023) \\\n",
    "    .groupBy(\"a.CompanyCode\") \\\n",
    "    .agg(\n",
    "        F.round(F.sum(F.col(\"a.TransactionAmount\") / F.col(\"b.ExchangeRate\")), 2).alias(\"total_transaction_amount_usd\")\n",
    "    )\n",
    "\n",
    "# Step 4: Show the result\n",
    "print(\"printing output result\")\n",
    "result_df.show()\n",
    "\n",
    "\n",
    "transactions_df = transactions_df.withColumn(\"DocumentDate\", F.to_date(\"DocumentDate\"))\n",
    "exchange_rates_df = exchange_rates_df.withColumn(\"EffectiveDate\", F.to_date(\"EffectiveDate\"))\n",
    "transactions_df.createOrReplaceTempView(\"SAP_Finance_system\")\n",
    "exchange_rates_df.createOrReplaceTempView(\"ExchangeRates\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "777fe7fa-465c-4521-9e08-002ef0a49241",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>CompanyCode</th><th>total_transaction_amount_usd</th></tr></thead><tbody><tr><td>US01</td><td>500.0</td></tr><tr><td>DE02</td><td>1045.45</td></tr><tr><td>IN03</td><td>2916666.67</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "US01",
         500.0
        ],
        [
         "DE02",
         1045.45
        ],
        [
         "IN03",
         2916666.67
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "CompanyCode",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "total_transaction_amount_usd",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "WITH LatestRates AS (\n",
    "    SELECT Currency, ExchangeRate, EffectiveDate,\n",
    "           ROW_NUMBER() OVER (PARTITION BY Currency ORDER BY EffectiveDate DESC) AS rn\n",
    "    FROM ExchangeRates\n",
    ")\n",
    "SELECT a.CompanyCode,\n",
    "       ROUND(SUM(a.TransactionAmount / b.ExchangeRate), 2) AS total_transaction_amount_usd\n",
    "FROM SAP_Finance_system a\n",
    "LEFT JOIN LatestRates b\n",
    "ON a.Currency = b.Currency\n",
    "AND b.EffectiveDate <= a.DocumentDate\n",
    "AND b.rn = 1\n",
    "WHERE a.FiscalYear = 2023\n",
    "GROUP BY a.CompanyCode;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30782a89-340c-412e-bb10-80f04985ebe3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## QUESTION 30 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b47f00fd-06a9-463a-bd6c-60b290a23e0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Write a pyspark and sql query to identify customers who had any deliveries delayed by more than 3 days. For these\n",
    "customers, calculate the percentage of their total orders that were delayed. The result should\n",
    "include the customer ID, the number of delayed orders, total orders, and the delay percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "950f5428-4d2b-43d2-8096-004aad845eaf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+----------+------------+------------+\n|sales_order_id|customer_id|order_date|order_status|total_amount|\n+--------------+-----------+----------+------------+------------+\n|          1001|        201|2023-05-01|   Completed|      1500.0|\n|          1002|        202|2023-05-02|   Completed|      2000.0|\n|          1003|        203|2023-05-03|   Completed|      3000.0|\n|          1004|        204|2023-05-05|   Completed|      2500.0|\n|          1005|        205|2023-05-07|   Cancelled|         0.0|\n+--------------+-----------+----------+------------+------------+\n\n+-----------+--------------+-------------+---------------+-------------+\n|delivery_id|sales_order_id|delivery_date|delivery_status|delivery_time|\n+-----------+--------------+-------------+---------------+-------------+\n|       5001|          1001|   2023-05-03|      Delivered|          2.0|\n|       5002|          1002|   2023-05-06|      Delivered|          4.0|\n|       5003|          1003|   2023-05-05|      Delivered|          2.0|\n|       5004|          1004|   2023-05-09|      Delivered|          4.0|\n|       5005|          1001|   2023-05-04|       Returned|         null|\n+-----------+--------------+-------------+---------------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType, FloatType\n",
    "\n",
    "\n",
    "# Define schema for SalesOrders table\n",
    "sales_orders_schema = StructType([\n",
    "    StructField(\"sales_order_id\", IntegerType(), True),\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"order_date\", StringType(), True),\n",
    "    StructField(\"order_status\", StringType(), True),\n",
    "    StructField(\"total_amount\", FloatType(), True)\n",
    "])\n",
    "\n",
    "# Create SalesOrders DataFrame\n",
    "sales_orders_data = [\n",
    "    (1001, 201, \"2023-05-01\", \"Completed\", 1500.0),\n",
    "    (1002, 202, \"2023-05-02\", \"Completed\", 2000.0),\n",
    "    (1003, 203, \"2023-05-03\", \"Completed\", 3000.0),\n",
    "    (1004, 204, \"2023-05-05\", \"Completed\", 2500.0),\n",
    "    (1005, 205, \"2023-05-07\", \"Cancelled\", 0.0)\n",
    "]\n",
    "\n",
    "sales_orders_df = spark.createDataFrame(data=sales_orders_data, schema=sales_orders_schema)\n",
    "\n",
    "# Define schema for Deliveries table\n",
    "deliveries_schema = StructType([\n",
    "    StructField(\"delivery_id\", IntegerType(), True),\n",
    "    StructField(\"sales_order_id\", IntegerType(), True),\n",
    "    StructField(\"delivery_date\", StringType(), True),\n",
    "    StructField(\"delivery_status\", StringType(), True),\n",
    "    StructField(\"delivery_time\", FloatType(), True)\n",
    "])\n",
    "\n",
    "# Create Deliveries DataFrame\n",
    "deliveries_data = [\n",
    "    (5001, 1001, \"2023-05-03\", \"Delivered\", 2.0),\n",
    "    (5002, 1002, \"2023-05-06\", \"Delivered\", 4.0),\n",
    "    (5003, 1003, \"2023-05-05\", \"Delivered\", 2.0),\n",
    "    (5004, 1004, \"2023-05-09\", \"Delivered\", 4.0),\n",
    "    (5005, 1001, \"2023-05-04\", \"Returned\", None)\n",
    "]\n",
    "\n",
    "deliveries_df = spark.createDataFrame(data=deliveries_data, schema=deliveries_schema)\n",
    "\n",
    "# Display the DataFrames\n",
    "sales_orders_df.show()\n",
    "sales_orders_df.createOrReplaceTempView('SalesOrders')\n",
    "deliveries_df.show()\n",
    "deliveries_df.createOrReplaceTempView('Deliveries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "040d782b-e9ef-4a44-80a1-4b126274365b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+------------+----------------+\n|customer_id|delayed_orders|total_orders|delay_percentage|\n+-----------+--------------+------------+----------------+\n|        201|             0|           1|             0.0|\n|        202|             1|           1|           100.0|\n|        203|             0|           1|             0.0|\n|        204|             1|           1|           100.0|\n+-----------+--------------+------------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Step 1: Identify delayed deliveries (more than 3 days)\n",
    "delayed_deliveries_df = (\n",
    "    sales_orders_df.alias(\"so\")\n",
    "    .join(deliveries_df.alias(\"d\"), F.col(\"so.sales_order_id\") == F.col(\"d.sales_order_id\"))\n",
    "    .filter((F.col(\"d.delivery_time\") > 3) & (F.col(\"so.order_status\") == \"Completed\"))\n",
    "    .groupBy(\"so.customer_id\")\n",
    "    .agg(F.countDistinct(\"so.sales_order_id\").alias(\"delayed_orders\"))\n",
    ")\n",
    "\n",
    "# Step 2: Count total completed orders per customer\n",
    "total_orders_df = (\n",
    "    sales_orders_df\n",
    "    .filter(F.col(\"order_status\") == \"Completed\")\n",
    "    .groupBy(\"customer_id\")\n",
    "    .agg(F.count(\"sales_order_id\").alias(\"total_orders\"))\n",
    ")\n",
    "\n",
    "# Step 3: Calculate the delay percentage\n",
    "result_df = (\n",
    "    total_orders_df.alias(\"t\")\n",
    "    .join(delayed_deliveries_df.alias(\"d\"), F.col(\"t.customer_id\") == F.col(\"d.customer_id\"), \"left\")\n",
    "    .select(\n",
    "        F.col(\"t.customer_id\"),\n",
    "        F.coalesce(F.col(\"d.delayed_orders\"), F.lit(0)).alias(\"delayed_orders\"),\n",
    "        F.col(\"t.total_orders\"),\n",
    "        F.round(F.coalesce((F.col(\"d.delayed_orders\") * 100.0) / F.col(\"t.total_orders\"), F.lit(0)), 2).alias(\"delay_percentage\")\n",
    "    )\n",
    "    .filter(F.col(\"t.total_orders\") > 0)\n",
    ")\n",
    "\n",
    "# Display the final result\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f533e01-7361-4d48-9b00-d43e26f11bf2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>customer_id</th><th>delayed_orders</th><th>total_orders</th><th>delay_percentage</th></tr></thead><tbody><tr><td>201</td><td>0</td><td>1</td><td>0.00</td></tr><tr><td>202</td><td>1</td><td>1</td><td>100.00</td></tr><tr><td>203</td><td>0</td><td>1</td><td>0.00</td></tr><tr><td>204</td><td>1</td><td>1</td><td>100.00</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         201,
         0,
         1,
         "0.00"
        ],
        [
         202,
         1,
         1,
         "100.00"
        ],
        [
         203,
         0,
         1,
         "0.00"
        ],
        [
         204,
         1,
         1,
         "100.00"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "customer_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "delayed_orders",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "total_orders",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "delay_percentage",
         "type": "\"decimal(27,2)\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "WITH DelayedDeliveries AS (\n",
    "    -- Identify delayed deliveries (more than 3 days)\n",
    "    SELECT\n",
    "        so.customer_id,\n",
    "        COUNT(DISTINCT so.sales_order_id) AS delayed_orders\n",
    "    FROM SalesOrders AS so\n",
    "    JOIN Deliveries AS d\n",
    "        ON so.sales_order_id = d.sales_order_id\n",
    "    WHERE d.delivery_time > 3\n",
    "      AND so.order_status = 'Completed'\n",
    "    GROUP BY so.customer_id\n",
    "),\n",
    "TotalOrders AS (\n",
    "    -- Count total completed orders per customer\n",
    "    SELECT\n",
    "        customer_id,\n",
    "        COUNT(sales_order_id) AS total_orders\n",
    "    FROM SalesOrders\n",
    "    WHERE order_status = 'Completed'\n",
    "    GROUP BY customer_id\n",
    ")\n",
    "-- Final query: calculate the delay percentage\n",
    "SELECT\n",
    "    t.customer_id,\n",
    "    COALESCE(d.delayed_orders, 0) AS delayed_orders,\n",
    "    t.total_orders,\n",
    "    ROUND(COALESCE((d.delayed_orders * 100.0) / t.total_orders, 0), 2) AS delay_percentage\n",
    "FROM TotalOrders AS t\n",
    "LEFT JOIN DelayedDeliveries AS d\n",
    "    ON t.customer_id = d.customer_id\n",
    "WHERE t.total_orders > 0;\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2277322503791257,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Spark_Case_Based_Coding_Question",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
